#!/usr/bin/env python3
"""
Teste do Sistema de Tracking de M√©tricas

Este teste demonstra o sistema completo de coleta,
an√°lise e monitoramento de m√©tricas em tempo real.
"""

import asyncio
import random
import time
import sys
import os
from pathlib import Path

# Adicionar diret√≥rio pai ao path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.metrics_tracker import (
    MetricsTracker, MetricType, AlertRule, AlertSeverity,
    TimerContext, track_timing, setup_default_alerts
)


class MockScraper:
    """Simulador de scraper para gerar m√©tricas"""
    
    def __init__(self):
        self.metrics = MetricsTracker("test_metrics")
        self.page_count = 0
        self.job_count = 0
    
    @track_timing("scraper.page_processing_time")
    async def process_page(self, page_num: int, failure_rate: float = 0.2):
        """Simula processamento de uma p√°gina"""
        print(f"üìÑ Processando p√°gina {page_num}...")
        
        # Simular tempo de carregamento vari√°vel
        processing_time = random.uniform(0.5, 2.0)
        await asyncio.sleep(processing_time)
        
        # Simular falha ocasional
        if random.random() < failure_rate:
            self.metrics.increment_counter("scraper.pages_failed")
            raise Exception(f"Falha ao processar p√°gina {page_num}")
        
        # Sucesso
        self.page_count += 1
        jobs_found = random.randint(8, 25)
        
        self.metrics.increment_counter("scraper.pages_processed")
        self.metrics.increment_counter("scraper.jobs_found", jobs_found)
        self.metrics.record_timer("scraper.page_load_time", processing_time)
        
        return jobs_found
    
    @track_timing("scraper.job_processing_time")
    async def process_job(self, job_id: int, failure_rate: float = 0.15):
        """Simula processamento de uma vaga"""
        # Simular tempo de processamento
        processing_time = random.uniform(0.1, 0.8)
        await asyncio.sleep(processing_time)
        
        # Simular falha ocasional
        if random.random() < failure_rate:
            self.metrics.increment_counter("scraper.jobs_failed")
            self.metrics.increment_counter("scraper.processing_errors")
            raise Exception(f"Falha ao processar job {job_id}")
        
        # Sucesso
        self.job_count += 1
        self.metrics.increment_counter("scraper.jobs_processed")
        
        # Simular qualidade dos dados vari√°vel
        quality_score = random.uniform(70, 95)
        self.metrics.set_gauge("validation.quality_score", quality_score)
        
        return {
            'id': job_id,
            'titulo': f'Desenvolvedor Python {job_id}',
            'quality_score': quality_score
        }
    
    async def run_scraping_session(self, pages: int = 5, concurrent_jobs: int = 3):
        """Simula uma sess√£o completa de scraping"""
        print(f"üöÄ Iniciando sess√£o de scraping: {pages} p√°ginas, {concurrent_jobs} jobs paralelos")
        
        with TimerContext("scraper.total_session_time"):
            all_jobs = []
            
            # Processar p√°ginas sequencialmente
            for page_num in range(1, pages + 1):
                try:
                    jobs_found = await self.process_page(page_num)
                    print(f"   ‚úÖ P√°gina {page_num}: {jobs_found} vagas encontradas")
                    
                    # Processar jobs em paralelo
                    semaphore = asyncio.Semaphore(concurrent_jobs)
                    
                    async def process_with_semaphore(job_id):
                        async with semaphore:
                            return await self.process_job(job_id)
                    
                    # Criar tasks para jobs desta p√°gina
                    job_tasks = []
                    for job_id in range(len(all_jobs), len(all_jobs) + jobs_found):
                        task = process_with_semaphore(job_id)
                        job_tasks.append(task)
                    
                    # Executar jobs em paralelo
                    page_jobs = await asyncio.gather(*job_tasks, return_exceptions=True)
                    
                    # Filtrar sucessos
                    successful_jobs = [job for job in page_jobs if not isinstance(job, Exception)]
                    all_jobs.extend(successful_jobs)
                    
                    print(f"   üìä P√°gina {page_num}: {len(successful_jobs)}/{jobs_found} jobs processados")
                    
                except Exception as e:
                    print(f"   ‚ùå Falha na p√°gina {page_num}: {e}")
                    continue
                
                # Pequena pausa entre p√°ginas
                await asyncio.sleep(0.2)
            
            # Calcular m√©tricas finais
            total_jobs_found = self.metrics.get_metric_summary("scraper.jobs_found")
            total_jobs_processed = self.metrics.get_metric_summary("scraper.jobs_processed")
            
            if total_jobs_found and total_jobs_processed:
                success_rate = (total_jobs_processed.last_value / total_jobs_found.last_value) * 100
                error_rate = 100 - success_rate
                
                self.metrics.set_gauge("scraper.success_rate", success_rate)
                self.metrics.set_gauge("scraper.error_rate", error_rate)
                
                print(f"\nüìä SESS√ÉO CONCLU√çDA:")
                print(f"   üìà Taxa de sucesso: {success_rate:.1f}%")
                print(f"   üìâ Taxa de erro: {error_rate:.1f}%")
                print(f"   üì¶ Jobs processados: {int(total_jobs_processed.last_value)}")
            
            return all_jobs


async def test_basic_metrics():
    """Teste b√°sico de m√©tricas"""
    print("üß™ TESTE B√ÅSICO DE M√âTRICAS")
    print("-" * 50)
    
    metrics = MetricsTracker("test_basic")
    
    # Registrar diferentes tipos de m√©tricas
    metrics.register_metric("test.counter", MetricType.COUNTER)
    metrics.register_metric("test.gauge", MetricType.GAUGE)
    metrics.register_metric("test.timer", MetricType.TIMER)
    
    # Gerar dados de teste
    for i in range(20):
        metrics.increment_counter("test.counter", random.uniform(1, 5))
        metrics.set_gauge("test.gauge", random.uniform(0, 100))
        metrics.record_timer("test.timer", random.uniform(0.1, 2.0))
        
        await asyncio.sleep(0.05)  # Simular atividade
    
    # Exibir resumos
    print("\nüìä RESUMOS DAS M√âTRICAS:")
    summaries = metrics.get_all_summaries()
    
    for name, summary in summaries.items():
        if summary.count > 0:
            print(f"   ‚Ä¢ {name}:")
            print(f"     - Tipo: {summary.metric_type.value}")
            print(f"     - Count: {summary.count}")
            print(f"     - M√©dia: {summary.avg_value:.3f}")
            print(f"     - Min/Max: {summary.min_value:.3f}/{summary.max_value:.3f}")
            print(f"     - P95: {summary.percentile_95:.3f}")
    
    print("‚úÖ Teste b√°sico conclu√≠do!")


async def test_alerts_system():
    """Teste do sistema de alertas"""
    print("\nüß™ TESTE DO SISTEMA DE ALERTAS")
    print("-" * 50)
    
    metrics = MetricsTracker("test_alerts")
    
    # Configurar alertas de teste
    def alert_callback(alert):
        print(f"üö® CALLBACK EXECUTADO: {alert.message}")
    
    test_rules = [
        AlertRule(
            name="high_error_rate",
            metric_name="test.error_rate",
            condition="gt",
            threshold=20.0,
            severity=AlertSeverity.HIGH,
            callback=alert_callback,
            cooldown=1.0  # 1 segundo para teste
        ),
        AlertRule(
            name="low_success_rate",
            metric_name="test.success_rate", 
            condition="lt",
            threshold=80.0,
            severity=AlertSeverity.MEDIUM,
            cooldown=1.0
        ),
        AlertRule(
            name="critical_failure",
            metric_name="test.critical_metric",
            condition="gt",
            threshold=5.0,
            severity=AlertSeverity.CRITICAL,
            cooldown=1.0
        )
    ]
    
    for rule in test_rules:
        metrics.add_alert_rule(rule)
    
    # Simular condi√ß√µes normais
    print("Fase 1: Condi√ß√µes normais...")
    for i in range(5):
        metrics.set_gauge("test.error_rate", random.uniform(5, 15))  # Normal
        metrics.set_gauge("test.success_rate", random.uniform(85, 95))  # Normal
        await asyncio.sleep(0.2)
    
    # Simular deteriora√ß√£o gradual
    print("\nFase 2: Deteriora√ß√£o gradual...")
    for i in range(8):
        error_rate = 10 + (i * 3)  # Aumentando gradualmente
        success_rate = 95 - (i * 3)  # Diminuindo gradualmente
        
        metrics.set_gauge("test.error_rate", error_rate)
        metrics.set_gauge("test.success_rate", success_rate)
        
        print(f"   M√©tricas: Erro={error_rate:.1f}%, Sucesso={success_rate:.1f}%")
        await asyncio.sleep(0.3)
    
    # Simular condi√ß√£o cr√≠tica
    print("\nFase 3: Condi√ß√£o cr√≠tica...")
    for i in range(3):
        metrics.set_gauge("test.critical_metric", 10.0)  # Acima do threshold
        await asyncio.sleep(0.5)
    
    # Mostrar alertas gerados
    print(f"\nüìä ALERTAS GERADOS: {len(metrics.active_alerts)}")
    for alert in metrics.active_alerts:
        severity_icons = {
            AlertSeverity.LOW: "üîµ",
            AlertSeverity.MEDIUM: "üü°", 
            AlertSeverity.HIGH: "üî¥",
            AlertSeverity.CRITICAL: "üíÄ"
        }
        icon = severity_icons.get(alert.severity, "‚ö™")
        print(f"   {icon} {alert.severity.value.upper()}: {alert.message}")
    
    print("‚úÖ Teste de alertas conclu√≠do!")


async def test_dashboard_and_export():
    """Teste de dashboard e exporta√ß√£o"""
    print("\nüß™ TESTE DE DASHBOARD E EXPORTA√á√ÉO")
    print("-" * 50)
    
    scraper = MockScraper()
    
    # Iniciar monitoramento em background
    scraper.metrics.start_background_monitoring()
    
    # Configurar alertas padr√£o
    setup_default_alerts()
    
    # Executar sess√£o de scraping simulada
    jobs = await scraper.run_scraping_session(pages=3, concurrent_jobs=2)
    
    print(f"\nüìä RESULTADOS DA SIMULA√á√ÉO:")
    print(f"   üì¶ Jobs coletados: {len(jobs)}")
    
    # Mostrar dashboard
    scraper.metrics.print_dashboard()
    
    # Testar exporta√ß√£o
    print("\nüìÅ TESTANDO EXPORTA√á√ÉO:")
    
    json_file = scraper.metrics.export_metrics('json')
    csv_file = scraper.metrics.export_metrics('csv')
    
    print(f"   üìÑ JSON exportado: {json_file}")
    print(f"   üìÑ CSV exportado: {csv_file}")
    
    # Verificar se arquivos foram criados
    if Path(json_file).exists():
        size = Path(json_file).stat().st_size
        print(f"   ‚úÖ JSON criado: {size} bytes")
    
    if Path(csv_file).exists():
        size = Path(csv_file).stat().st_size
        print(f"   ‚úÖ CSV criado: {size} bytes")
    
    # Parar monitoramento
    scraper.metrics.stop_background_monitoring()
    
    print("‚úÖ Teste de dashboard conclu√≠do!")


async def test_performance_tracking():
    """Teste de tracking de performance"""
    print("\nüß™ TESTE DE PERFORMANCE TRACKING")
    print("-" * 50)
    
    metrics = MetricsTracker("test_performance")
    
    # Simular diferentes padr√µes de performance
    operations = {
        "fast_operation": (0.05, 0.15),    # 50-150ms
        "medium_operation": (0.2, 0.5),    # 200-500ms  
        "slow_operation": (1.0, 3.0),      # 1-3s
        "variable_operation": (0.1, 2.0)   # Muito vari√°vel
    }
    
    print("Executando opera√ß√µes com diferentes perfis de performance...")
    
    for operation_name, (min_time, max_time) in operations.items():
        print(f"\nüìä Testando: {operation_name}")
        
        # Executar m√∫ltiplas vezes para gerar estat√≠sticas
        for i in range(15):
            duration = random.uniform(min_time, max_time)
            
            # Adicionar alguns outliers ocasionais
            if random.random() < 0.1:  # 10% chance
                duration *= random.uniform(2, 5)  # 2-5x mais lento
            
            metrics.record_timer(f"performance.{operation_name}", duration)
            
            # Simular labels para segmenta√ß√£o
            labels = {
                "environment": random.choice(["prod", "staging", "dev"]),
                "region": random.choice(["us-east", "us-west", "eu-central"])
            }
            metrics.record_timer(f"performance.{operation_name}.labeled", duration, labels)
            
            await asyncio.sleep(0.02)  # Simular intervalo
    
    # An√°lise de performance
    print(f"\nüìä AN√ÅLISE DE PERFORMANCE:")
    summaries = metrics.get_all_summaries()
    
    perf_metrics = {name: summary for name, summary in summaries.items() 
                   if name.startswith("performance.") and not ".labeled" in name}
    
    # Ordenar por tempo m√©dio
    sorted_metrics = sorted(perf_metrics.items(), key=lambda x: x[1].avg_value, reverse=True)
    
    for name, summary in sorted_metrics:
        op_name = name.replace("performance.", "")
        print(f"   üîç {op_name}:")
        print(f"      - Tempo m√©dio: {summary.avg_value:.3f}s")
        print(f"      - P95: {summary.percentile_95:.3f}s")
        print(f"      - P99: {summary.percentile_99:.3f}s")
        print(f"      - Min/Max: {summary.min_value:.3f}s / {summary.max_value:.3f}s")
        print(f"      - Std Dev: {summary.std_dev:.3f}s")
        
        # Classifica√ß√£o de performance
        if summary.avg_value < 0.2:
            classification = "üü¢ R√ÅPIDA"
        elif summary.avg_value < 1.0:
            classification = "üü° M√âDIA"
        else:
            classification = "üî¥ LENTA"
        
        print(f"      - Classifica√ß√£o: {classification}")
    
    print("‚úÖ Teste de performance conclu√≠do!")


async def test_concurrent_metrics():
    """Teste de m√©tricas concorrentes"""
    print("\nüß™ TESTE DE M√âTRICAS CONCORRENTES")
    print("-" * 50)
    
    metrics = MetricsTracker("test_concurrent")
    
    async def worker(worker_id: int, iterations: int):
        """Worker que gera m√©tricas em paralelo"""
        for i in range(iterations):
            # Diferentes tipos de m√©tricas por worker
            metrics.increment_counter(f"worker.{worker_id}.operations", 1)
            metrics.set_gauge(f"worker.{worker_id}.status", random.uniform(0, 100))
            
            # M√©tricas compartilhadas
            metrics.increment_counter("global.total_operations", 1)
            
            with TimerContext(f"worker.{worker_id}.operation_time"):
                # Simular trabalho
                await asyncio.sleep(random.uniform(0.01, 0.1))
            
            # Labels para segmenta√ß√£o
            labels = {"worker_id": str(worker_id), "batch": str(i // 5)}
            metrics.record_timer("global.operation_time", random.uniform(0.01, 0.1), labels)
    
    # Executar m√∫ltiplos workers concorrentemente
    print("Executando 5 workers concorrentes...")
    
    tasks = []
    for worker_id in range(5):
        task = worker(worker_id, 20)
        tasks.append(task)
    
    await asyncio.gather(*tasks)
    
    # An√°lise dos resultados
    print(f"\nüìä AN√ÅLISE DE CONCORR√äNCIA:")
    summaries = metrics.get_all_summaries()
    
    # M√©tricas por worker
    worker_metrics = {name: summary for name, summary in summaries.items() 
                     if name.startswith("worker.")}
    
    print(f"   üìà Opera√ß√µes globais: {int(summaries['global.total_operations'].last_value)}")
    print(f"   ‚è±Ô∏è Tempo m√©dio global: {summaries['global.operation_time'].avg_value:.3f}s")
    
    # Performance por worker
    worker_ops = {}
    for name, summary in worker_metrics.items():
        if ".operations" in name:
            worker_id = name.split(".")[1]
            worker_ops[worker_id] = int(summary.last_value)
    
    print(f"   üë• Performance por worker:")
    for worker_id, ops in sorted(worker_ops.items()):
        print(f"      - Worker {worker_id}: {ops} opera√ß√µes")
    
    print("‚úÖ Teste de concorr√™ncia conclu√≠do!")


async def main():
    """Fun√ß√£o principal dos testes"""
    print("üß™ TESTES COMPLETOS DO SISTEMA DE M√âTRICAS")
    print("=" * 80)
    print("Este conjunto de testes demonstra coleta, an√°lise e")
    print("monitoramento avan√ßado de m√©tricas em tempo real.\n")
    
    try:
        await test_basic_metrics()
        await test_alerts_system()
        await test_dashboard_and_export()
        await test_performance_tracking()
        await test_concurrent_metrics()
        
        print("\n" + "=" * 80)
        print("üéâ TODOS OS TESTES DE M√âTRICAS CONCLU√çDOS!")
        print("=" * 80)
        
        print("\nüí° FUNCIONALIDADES DEMONSTRADAS:")
        print("   ‚úÖ Coleta de m√©tricas multi-tipo (counter, gauge, timer, histogram)")
        print("   ‚úÖ Sistema de alertas configur√°vel com callbacks")
        print("   ‚úÖ Dashboard em tempo real com an√°lise estat√≠stica")
        print("   ‚úÖ Exporta√ß√£o para JSON e CSV")
        print("   ‚úÖ Tracking de performance com percentis")
        print("   ‚úÖ M√©tricas concorrentes thread-safe")
        print("   ‚úÖ Agrega√ß√£o e segmenta√ß√£o por labels")
        print("   ‚úÖ An√°lise de tend√™ncias e sa√∫de do sistema")
        print("   ‚úÖ Context managers e decorators para automa√ß√£o")
        
        print("\nüìä BENEF√çCIOS PARA PRODU√á√ÉO:")
        print("   üîç Visibilidade completa do sistema")
        print("   üö® Detec√ß√£o proativa de problemas")
        print("   üìà An√°lise de performance e otimiza√ß√£o")
        print("   üìã Relat√≥rios autom√°ticos e exporta√ß√£o")
        print("   ‚ö° Monitoramento em tempo real")
        
        print("\nüöÄ SISTEMA DE M√âTRICAS PRONTO PARA PRODU√á√ÉO!")
        
    except Exception as e:
        print(f"\n‚ùå Erro durante testes: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())