"""
Testes para validar as correÃ§Ãµes crÃ­ticas implementadas
"""

import asyncio
import time
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime, timedelta

# Importar componentes a serem testados
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.systems.connection_pool import ConnectionPool, PooledPage
from src.systems.incremental_processor import IncrementalProcessor
from src.utils.lru_cache import LRUCacheWithTTL, CVJobMatcherCache


class TestConnectionPoolFixes:
    """Testa correÃ§Ãµes de race conditions no connection pool"""
    
    async def test_connection_pool_thread_safety(self):
        """Testa se pool Ã© thread-safe apÃ³s correÃ§Ãµes"""
        
        # Mock do browser
        mock_browser = Mock()
        mock_page = Mock()
        mock_context = Mock()
        mock_page.context = mock_context
        mock_page.goto = AsyncMock()
        mock_page.evaluate = AsyncMock()
        mock_context.clear_cookies = AsyncMock()
        mock_browser.new_page = AsyncMock(return_value=mock_page)
        
        pool = ConnectionPool(min_size=2, max_size=5)
        
        # Inicializar pool
        await pool.initialize(mock_browser)
        
        # Simular acesso concorrente
        async def concurrent_access():
            page = await pool.get_page(timeout_seconds=1.0)
            if page:
                await asyncio.sleep(0.01)  # Simular uso
                await pool.return_page(page)
                return True
            return False
        
        # Executar mÃºltiplas operaÃ§Ãµes concorrentes
        tasks = [concurrent_access() for _ in range(10)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Verificar que nÃ£o houve exceÃ§Ãµes
        exceptions = [r for r in results if isinstance(r, Exception)]
        assert len(exceptions) == 0, f"Encontradas exceÃ§Ãµes: {exceptions}"
        
        # Verificar consistÃªncia do pool
        stats = pool.get_stats()
        assert stats['available_pages'] >= 0
        assert stats['busy_pages'] >= 0
        assert stats['available_pages'] + stats['busy_pages'] <= pool.max_size
        
        await pool.shutdown()
    
    async def test_connection_pool_timeout_handling(self):
        """Testa se timeouts sÃ£o tratados corretamente"""
        
        mock_browser = Mock()
        mock_browser.new_page = AsyncMock(side_effect=Exception("Timeout simulado"))
        
        pool = ConnectionPool(min_size=1, max_size=2)
        
        # Tentar inicializar com erro
        await pool.initialize(mock_browser)
        
        # Tentar obter pÃ¡gina com timeout curto
        start_time = time.time()
        page = await pool.get_page(timeout_seconds=0.1)
        elapsed = time.time() - start_time
        
        # Deve retornar None e respeitar timeout
        assert page is None
        assert elapsed < 0.5  # NÃ£o deve demorar muito mais que o timeout
        
        await pool.shutdown()


class TestIncrementalProcessorFixes:
    """Testa correÃ§Ãµes no processador incremental"""
    
    def test_absolute_page_limit(self):
        """Testa se limite absoluto de pÃ¡ginas funciona"""
        
        processor = IncrementalProcessor()
        
        # Simular vagas da pÃ¡gina
        fake_jobs = [{'id': f'job_{i}', 'title': f'Job {i}'} for i in range(10)]
        
        # Testar limite absoluto
        should_continue, new_jobs = processor.should_continue_processing(
            current_page_jobs=fake_jobs,
            threshold=0.0,  # Threshold baixo para forÃ§ar continuaÃ§Ã£o
            page_number=51,  # Acima do limite padrÃ£o de 50
            max_absolute_pages=50
        )
        
        # Deve parar devido ao limite absoluto
        assert should_continue is False
        assert 'stopped_reason' in processor.session_stats
        assert processor.session_stats['stopped_reason'] == 'absolute_limit'
    
    def test_division_by_zero_protection(self):
        """Testa proteÃ§Ã£o contra divisÃ£o por zero"""
        
        processor = IncrementalProcessor()
        
        # Teste com lista vazia
        should_continue, new_jobs = processor.should_continue_processing(
            current_page_jobs=[],
            threshold=0.5,
            page_number=1
        )
        
        # Deve retornar True para lista vazia
        assert should_continue is True
        assert new_jobs == []
        
    def test_threshold_calculations(self):
        """Testa cÃ¡lculos de threshold estÃ£o corretos"""
        
        processor = IncrementalProcessor()
        
        # Mock do mÃ©todo is_job_processed para simular jobs conhecidos
        with patch.object(processor, 'is_job_processed') as mock_is_processed:
            # Simular que jobs com id terminado em 0 sÃ£o conhecidos
            mock_is_processed.side_effect = lambda job: job['id'].endswith('0')
            
            fake_jobs = [{'id': f'job_{i}'} for i in range(10)]
            
            should_continue, new_jobs = processor.should_continue_processing(
                current_page_jobs=fake_jobs,
                threshold=0.5,
                page_number=5,
                max_absolute_pages=50
            )
            
            # Com jobs 0 conhecidos e resto novos: job_0 Ã© conhecido = 1 conhecido, 9 novos
            assert len(new_jobs) == 9  # jobs 1-9 sÃ£o novos
            # Na pÃ¡gina 5 (6-10), usa threshold * 0.5 = 0.25
            # Ratio = 9/10 = 0.9 > 0.25, entÃ£o deve continuar
            assert should_continue is True


class TestLRUCacheFixes:
    """Testa o novo sistema de cache com TTL"""
    
    def test_lru_cache_basic_operations(self):
        """Testa operaÃ§Ãµes bÃ¡sicas do cache LRU"""
        
        cache = LRUCacheWithTTL(max_size=3, ttl_seconds=1)
        
        # Teste de set/get
        cache.set('key1', 'value1')
        assert cache.get('key1') == 'value1'
        
        # Teste de capacidade mÃ¡xima
        cache.set('key2', 'value2')
        cache.set('key3', 'value3')
        cache.set('key4', 'value4')  # Deve remover key1
        
        assert cache.get('key1') is None  # Foi removido
        assert cache.get('key4') == 'value4'  # Foi adicionado
        assert len(cache) == 3
    
    def test_lru_cache_ttl_expiration(self):
        """Testa expiraÃ§Ã£o por TTL"""
        
        cache = LRUCacheWithTTL(max_size=10, ttl_seconds=0.1)
        
        cache.set('key1', 'value1')
        assert cache.get('key1') == 'value1'
        
        # Esperar expirar
        time.sleep(0.15)
        
        assert cache.get('key1') is None  # Deve ter expirado
    
    def test_cv_job_matcher_cache(self):
        """Testa cache especializado do CV Job Matcher"""
        
        cache = CVJobMatcherCache()
        
        # Teste de diferentes tipos de cache
        cache.set_cv('user1', {'name': 'JoÃ£o'})
        cache.set_job('job1', {'title': 'Developer'})
        cache.set_match('user1_job1', {'score': 0.8})
        
        assert cache.get_cv('user1')['name'] == 'JoÃ£o'
        assert cache.get_job('job1')['title'] == 'Developer'
        assert cache.get_match('user1_job1')['score'] == 0.8
        
        # Teste de estatÃ­sticas
        stats = cache.get_combined_stats()
        assert stats['total_size'] == 3
        assert stats['total_hits'] == 3
        assert stats['total_misses'] == 0
    
    def test_cache_thread_safety(self):
        """Testa thread safety do cache"""
        import threading
        
        cache = LRUCacheWithTTL(max_size=100, ttl_seconds=10)
        errors = []
        
        def worker(worker_id):
            try:
                for i in range(50):
                    key = f'worker_{worker_id}_key_{i}'
                    value = f'value_{i}'
                    cache.set(key, value)
                    retrieved = cache.get(key)
                    if retrieved != value:
                        errors.append(f"Worker {worker_id}: Expected {value}, got {retrieved}")
            except Exception as e:
                errors.append(f"Worker {worker_id}: Exception {e}")
        
        # Criar mÃºltiplas threads
        threads = []
        for i in range(5):
            t = threading.Thread(target=worker, args=(i,))
            threads.append(t)
            t.start()
        
        # Esperar todas terminarem
        for t in threads:
            t.join()
        
        # NÃ£o deve haver erros
        assert len(errors) == 0, f"Erros encontrados: {errors}"


class TestErrorHandlingFixes:
    """Testa melhorias no tratamento de erros"""
    
    def test_silent_failure_prevention(self):
        """Testa se falhas silenciosas foram eliminadas"""
        
        # Este teste verifica se os scrapers agora loggam erros de cleanup
        
        # Simular erro de cleanup
        error_messages = []
        
        def mock_print(msg):
            error_messages.append(msg)
        
        # Mock para simular o comportamento do scraper corrigido
        with patch('builtins.print', mock_print):
            # Simular cleanup com erros
            cleanup_errors = ["Erro ao fechar pÃ¡gina: Connection lost"]
            
            # CÃ³digo similar ao que foi implementado nos scrapers
            if cleanup_errors:
                mock_print(f"âš ï¸ Problemas durante cleanup: {len(cleanup_errors)} erros")
                for error in cleanup_errors[:3]:
                    mock_print(f"   â€¢ {error}")
        
        # Verificar que erros foram loggados
        assert len(error_messages) >= 2
        assert "Problemas durante cleanup" in error_messages[0]
        assert "Erro ao fechar pÃ¡gina" in error_messages[1]


# FunÃ§Ã£o para executar todos os testes
def run_critical_fixes_tests():
    """Executa todos os testes de correÃ§Ãµes crÃ­ticas"""
    
    print("ğŸ§ª Executando testes das correÃ§Ãµes crÃ­ticas...")
    
    # Testes sÃ­ncronos
    sync_test_classes = [
        TestIncrementalProcessorFixes,
        TestLRUCacheFixes,
        TestErrorHandlingFixes
    ]
    
    for test_class in sync_test_classes:
        print(f"\nğŸ“‹ Testando {test_class.__name__}...")
        instance = test_class()
        
        for method_name in dir(instance):
            if method_name.startswith('test_'):
                try:
                    print(f"   âœ“ {method_name}")
                    method = getattr(instance, method_name)
                    method()
                except Exception as e:
                    print(f"   âŒ {method_name}: {e}")
                    return False
    
    # Testes assÃ­ncronos
    async def run_async_tests():
        test_instance = TestConnectionPoolFixes()
        
        try:
            print(f"\nğŸ“‹ Testando {TestConnectionPoolFixes.__name__}...")
            print("   âœ“ test_connection_pool_thread_safety")
            await test_instance.test_connection_pool_thread_safety()
            
            print("   âœ“ test_connection_pool_timeout_handling") 
            await test_instance.test_connection_pool_timeout_handling()
            
        except Exception as e:
            print(f"   âŒ Teste assÃ­ncrono falhou: {e}")
            return False
        
        return True
    
    # Executar testes assÃ­ncronos
    try:
        result = asyncio.run(run_async_tests())
        if not result:
            return False
    except Exception as e:
        print(f"âŒ Erro ao executar testes assÃ­ncronos: {e}")
        return False
    
    print("\nâœ… Todos os testes das correÃ§Ãµes crÃ­ticas passaram!")
    return True


if __name__ == "__main__":
    success = run_critical_fixes_tests()
    exit(0 if success else 1)