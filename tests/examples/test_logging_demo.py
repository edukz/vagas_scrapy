#!/usr/bin/env python3
"""
Demo do sistema de logs estruturados em a√ß√£o
Mostra como os logs funcionam durante o scraping
"""

import asyncio
import time
import sys
import os

# Adicionar diret√≥rio atual ao path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.structured_logger import structured_logger, Component, LogLevel


async def simulate_scraping_with_logs():
    """Simula processo de scraping com logs estruturados"""
    print("üß™ DEMO DO SISTEMA DE LOGS ESTRUTURADOS")
    print("=" * 50)
    
    # Configurar console para mostrar mais logs
    structured_logger.set_console_level(LogLevel.INFO)
    
    # 1. Log in√≠cio da aplica√ß√£o
    structured_logger.info(
        "Demo application started",
        component=Component.MAIN,
        context={'demo_mode': True, 'version': '3.0'}
    )
    
    # 2. Simular in√≠cio de sess√£o de scraping
    with structured_logger.trace_operation("demo_scraping_session", Component.SCRAPER):
        config = {
            'max_pages': 2,
            'concurrent_jobs': 3,
            'url': 'catho.com.br/vagas/home-office'
        }
        
        session_trace_id = structured_logger.log_scraping_session_start(config)
        
        # 3. Simular navega√ß√£o por p√°ginas
        for page_num in range(1, 3):
            with structured_logger.track_performance(Component.SCRAPER, f"scrape_page_{page_num}") as tracker:
                structured_logger.scraper_log(
                    f"Starting page {page_num} extraction",
                    operation=f"scrape_page_{page_num}",
                    url=f"catho.com.br/vagas/home-office/?page={page_num}",
                    context={'page_number': page_num}
                )
                
                # Simular extra√ß√£o de jobs
                await asyncio.sleep(0.5)  # Simular tempo de carregamento
                
                jobs_found = 15 if page_num == 1 else 12
                structured_logger.scraper_log(
                    f"Page {page_num} extraction completed",
                    operation=f"scrape_page_{page_num}",
                    context={'jobs_found': jobs_found, 'page_number': page_num}
                )
        
        # 4. Simular sistema de retry
        print("\nüîÑ Simulando opera√ß√µes com retry...")
        
        # Retry bem-sucedido
        with structured_logger.track_performance(Component.RETRY_SYSTEM, "scrape_job_details"):
            for attempt in range(1, 4):
                structured_logger.retry_log(
                    f"Attempting to scrape job details (attempt {attempt})",
                    operation="scrape_job_details",
                    attempt=attempt,
                    max_attempts=3,
                    url="catho.com.br/vagas/dev-python/123456"
                )
                
                await asyncio.sleep(0.2)
                
                if attempt == 3:  # Sucesso na 3¬™ tentativa
                    structured_logger.retry_log(
                        "Job details scraped successfully after retry",
                        LogLevel.INFO,
                        operation="scrape_job_details",
                        attempt=attempt,
                        success=True,
                        duration_ms=600,
                        context={'retry_reason': 'timeout_error'}
                    )
                    break
                else:
                    structured_logger.retry_log(
                        f"Attempt {attempt} failed, retrying...",
                        LogLevel.WARN,
                        operation="scrape_job_details",
                        attempt=attempt,
                        retry_reason="timeout_error",
                        success=False,
                        error="Connection timeout after 30s"
                    )
        
        # 5. Simular fallback de seletores
        print("\nüéØ Simulando fallback de seletores...")
        
        structured_logger.fallback_log(
            "Primary selector failed, trying fallback",
            LogLevel.WARN,
            operation="extract_job_title",
            context={
                'primary_selector': 'h2.job-title',
                'fallback_selector': '[data-testid="job-title"]',
                'element_type': 'job_title'
            }
        )
        
        await asyncio.sleep(0.1)
        
        structured_logger.fallback_log(
            "Fallback selector succeeded",
            operation="extract_job_title",
            success=True,
            context={'selector_used': '[data-testid="job-title"]', 'fallback_level': 2}
        )
        
        # 6. Simular valida√ß√£o de dados
        print("\nüìã Simulando valida√ß√£o de dados...")
        
        # Valida√ß√£o de lote
        jobs_to_validate = 27
        structured_logger.validation_log(
            f"Starting batch validation of {jobs_to_validate} jobs",
            operation="validate_batch",
            context={'job_count': jobs_to_validate}
        )
        
        await asyncio.sleep(0.3)
        
        # Resultados da valida√ß√£o
        validation_results = {
            'total_jobs': jobs_to_validate,
            'valid_jobs': 22,
            'invalid_jobs': 5,
            'quality_score': 0.815,
            'corrections_applied': 34,
            'anomalies_detected': 3
        }
        
        structured_logger.validation_log(
            "Batch validation completed",
            operation="validate_batch",
            validation_score=validation_results['quality_score'],
            success=True,
            context=validation_results
        )
        
        # Exemplo de anomalia detectada
        structured_logger.validation_log(
            "Anomaly detected in job data",
            LogLevel.WARN,
            operation="detect_anomalies",
            anomaly_type="suspicious_title",
            context={
                'job_title': 'URGENTE!!! GANHE R$ 50.000 !!!',
                'reason': 'excessive_special_chars',
                'severity': 'medium'
            }
        )
        
        # 7. Simular cache
        print("\nüíæ Simulando opera√ß√µes de cache...")
        
        structured_logger.cache_log(
            "Cache hit for job details",
            operation="get_cached_job",
            context={
                'cache_key': 'job_123456',
                'hit_rate': 0.85,
                'ttl_remaining': 3600
            }
        )
        
        structured_logger.cache_log(
            "Cache miss, fetching from source",
            LogLevel.WARN,
            operation="get_cached_job",
            context={
                'cache_key': 'job_789012',
                'miss_reason': 'expired',
                'hit_rate': 0.85
            }
        )
        
        # 8. Finalizar sess√£o
        final_stats = {
            'total_jobs_found': jobs_to_validate,
            'valid_jobs': validation_results['valid_jobs'],
            'quality_score': validation_results['quality_score'],
            'total_pages_processed': 2,
            'cache_hit_rate': 0.85,
            'retry_operations': 1,
            'fallback_operations': 1,
            'total_duration_seconds': 2.1
        }
        
        structured_logger.log_scraping_session_end(final_stats)
    
    # 9. Mostrar estat√≠sticas dos logs
    print(f"\nüìä ESTAT√çSTICAS DOS LOGS GERADOS:")
    log_stats = structured_logger.get_log_stats()
    
    for log_type, stats in log_stats.items():
        if 'size_mb' in stats:
            print(f"   üìÑ {log_type}: {stats['lines']} linhas, {stats['size_mb']}MB")
    
    # 10. Mostrar exemplos de logs
    print(f"\nüìÅ ARQUIVOS DE LOG CRIADOS:")
    log_files = structured_logger.get_log_files()
    
    for log_type, file_path in log_files.items():
        if file_path.exists():
            print(f"   üìÑ {log_type}: {file_path}")
        
    print(f"\nüí° COMO VISUALIZAR OS LOGS:")
    print(f"   üìñ Logs principais: tail -f logs/scraper.log")
    print(f"   üîç Logs de debug: tail -f logs/scraper_debug.log") 
    print(f"   ‚ùå Apenas erros: tail -f logs/scraper_errors.log")
    
    print(f"\nüîç EXEMPLO DE LOG JSON:")
    main_log_file = log_files['main']
    if main_log_file.exists():
        with open(main_log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            if lines:
                import json
                try:
                    last_log = json.loads(lines[-1].strip())
                    print(f"   {json.dumps(last_log, indent=2, ensure_ascii=False)}")
                except:
                    print(f"   {lines[-1].strip()}")


async def main():
    """Fun√ß√£o principal do demo"""
    try:
        await simulate_scraping_with_logs()
        
        print("\n" + "=" * 50)
        print("üéâ DEMO CONCLU√çDO COM SUCESSO!")
        print("=" * 50)
        
        print("\nüí° BENEF√çCIOS DO SISTEMA DE LOGS:")
        print("   ‚úÖ Logs estruturados em JSON para an√°lise")
        print("   ‚úÖ Trace IDs para correlacionar opera√ß√µes")
        print("   ‚úÖ Performance tracking autom√°tico")
        print("   ‚úÖ Logs espec√≠ficos por componente")
        print("   ‚úÖ M√∫ltiplos arquivos por severidade")
        print("   ‚úÖ Rota√ß√£o autom√°tica para produ√ß√£o")
        print("   ‚úÖ Context rico para debugging")
        
        print("\nüöÄ O SISTEMA EST√Å PRONTO!")
        print("   Execute 'python main.py' para usar com logs completos")
        
    except Exception as e:
        print(f"\n‚ùå Erro durante demo: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())