"""
Sistema de Processamento Incremental

Este m√≥dulo implementa processamento incremental para o scraper,
permitindo processar apenas vagas novas e economizando at√© 90% do tempo
em execu√ß√µes subsequentes.

Funcionalidades:
- üéØ Detecta vagas j√° processadas e para quando as encontra
- üíæ Mant√©m checkpoint do √∫ltimo processamento
- üìä Estat√≠sticas de economia de tempo
- üîÑ Recupera√ß√£o autom√°tica em caso de falha
"""

import json
import os
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Set, Optional, Tuple
from pathlib import Path


class IncrementalProcessor:
    """
    Processador incremental para scraping eficiente
    
    Mant√©m registro das vagas j√° processadas e permite
    processar apenas novas vagas em execu√ß√µes subsequentes.
    """
    
    def __init__(self, checkpoint_dir: str = "data/checkpoints"):
        """
        Inicializa processador incremental
        
        Args:
            checkpoint_dir: Diret√≥rio para armazenar checkpoints
        """
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_file = os.path.join(checkpoint_dir, "incremental_checkpoint.json")
        self.stats_file = os.path.join(checkpoint_dir, "incremental_stats.json")
        
        # Criar diret√≥rio se n√£o existir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # Carregar checkpoint e estat√≠sticas
        self.checkpoint_data = self._load_checkpoint()
        self.stats_data = self._load_stats()
        
        # Estado da sess√£o atual
        self.session_start = datetime.now()
        self.session_stats = {
            'jobs_processed': 0,
            'jobs_skipped': 0,
            'pages_processed': 0,
            'pages_skipped': 0,
            'time_saved_seconds': 0
        }
    
    def _generate_job_id(self, job_data: Dict) -> str:
        """
        Gera ID √∫nico para uma vaga baseado em seus dados
        
        Usa uma combina√ß√£o de t√≠tulo, empresa e link para criar
        um identificador √∫nico e est√°vel.
        """
        # Campos usados para gerar o ID
        id_components = [
            job_data.get('titulo', ''),
            job_data.get('empresa', ''),
            job_data.get('link', ''),
            job_data.get('localizacao', '')
        ]
        
        # Criar string √∫nica
        id_string = '|'.join(str(comp) for comp in id_components)
        
        # Gerar hash MD5
        return hashlib.md5(id_string.encode()).hexdigest()
    
    def _load_checkpoint(self) -> Dict:
        """
        Carrega checkpoint do √∫ltimo processamento
        """
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # Converter lista de IDs para set para busca r√°pida
                if 'processed_job_ids' in data:
                    data['processed_job_ids'] = set(data['processed_job_ids'])
                
                print(f"‚úÖ Checkpoint carregado: {len(data.get('processed_job_ids', []))} vagas conhecidas")
                return data
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao carregar checkpoint: {e}")
        
        # Checkpoint padr√£o
        return {
            'last_run': None,
            'processed_job_ids': set(),
            'last_successful_page': 0,
            'total_jobs_processed': 0,
            'version': '1.0'
        }
    
    def _load_stats(self) -> Dict:
        """
        Carrega estat√≠sticas hist√≥ricas
        """
        if os.path.exists(self.stats_file):
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        # Estat√≠sticas padr√£o
        return {
            'total_runs': 0,
            'total_time_saved_minutes': 0,
            'average_jobs_per_run': 0,
            'average_time_saved_per_run': 0,
            'history': []
        }
    
    def _save_checkpoint(self) -> None:
        """
        Salva checkpoint atual
        """
        try:
            # Preparar dados para serializa√ß√£o
            checkpoint_data = self.checkpoint_data.copy()
            
            # Converter set para lista
            if 'processed_job_ids' in checkpoint_data:
                checkpoint_data['processed_job_ids'] = list(checkpoint_data['processed_job_ids'])
            
            # Salvar
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
            print(f"üíæ Checkpoint salvo: {len(self.checkpoint_data['processed_job_ids'])} vagas registradas")
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar checkpoint: {e}")
    
    def _save_stats(self) -> None:
        """
        Salva estat√≠sticas atualizadas
        """
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ùå Erro ao salvar estat√≠sticas: {e}")
    
    def is_job_processed(self, job_data: Dict) -> bool:
        """
        Verifica se uma vaga j√° foi processada anteriormente
        """
        job_id = self._generate_job_id(job_data)
        return job_id in self.checkpoint_data['processed_job_ids']
    
    def mark_job_processed(self, job_data: Dict) -> None:
        """
        Marca uma vaga como processada
        """
        job_id = self._generate_job_id(job_data)
        self.checkpoint_data['processed_job_ids'].add(job_id)
        self.checkpoint_data['total_jobs_processed'] += 1
        self.session_stats['jobs_processed'] += 1
    
    def should_continue_processing(self, current_page_jobs: List[Dict], 
                                  threshold: float = 0.5, page_number: int = 1) -> Tuple[bool, List[Dict]]:
        """
        Determina se deve continuar processando baseado em vagas j√° conhecidas
        
        Args:
            current_page_jobs: Lista de vagas da p√°gina atual
            threshold: Percentual de vagas novas necess√°rio para continuar (0.5 = 50%)
            page_number: N√∫mero da p√°gina atual (para estrat√©gia adaptativa)
            
        Returns:
            (should_continue, new_jobs): Se deve continuar e lista de vagas novas
        """
        if not current_page_jobs:
            return True, []
        
        new_jobs = []
        known_jobs = 0
        
        for job in current_page_jobs:
            if self.is_job_processed(job):
                known_jobs += 1
                self.session_stats['jobs_skipped'] += 1
            else:
                new_jobs.append(job)
        
        # Calcular propor√ß√£o de vagas novas
        new_ratio = len(new_jobs) / len(current_page_jobs)
        
        # Estrat√©gia adaptativa baseada na p√°gina
        if page_number <= 2:
            # P√°ginas 1-2: sempre continuar (para garantir que pelo menos tentamos p√°gina 2)
            should_continue = True
        elif page_number <= 5:
            # P√°ginas 3-5: continuar se tiver qualquer vaga nova ou ratio baixo
            should_continue = len(new_jobs) > 0 or new_ratio >= 0.05  # 5% threshold
        elif page_number <= 10:
            # P√°ginas 6-10: ser mais restritivo
            should_continue = new_ratio >= threshold * 0.5  # 50% do threshold
        else:
            # P√°ginas 10+: usar threshold completo
            should_continue = new_ratio >= threshold
        
        if not should_continue:
            print(f"üõë Parando processamento na p√°gina {page_number}: {known_jobs}/{len(current_page_jobs)} vagas j√° conhecidas (ratio: {new_ratio:.1%})")
            self.session_stats['time_saved_seconds'] = self._estimate_time_saved()
        else:
            if page_number <= 2:
                print(f"‚úÖ Continuando p√°gina {page_number}: {len(new_jobs)} vagas novas (pol√≠tica: sempre continuar nas 2 primeiras p√°ginas)")
            else:
                print(f"‚úÖ Continuando p√°gina {page_number}: {len(new_jobs)} vagas novas encontradas (ratio: {new_ratio:.1%})")
        
        return should_continue, new_jobs
    
    def process_page_incrementally(self, page_jobs: List[Dict], page_number: int) -> List[Dict]:
        """
        Processa uma p√°gina de forma incremental
        
        Returns:
            Lista de vagas novas (n√£o processadas anteriormente)
        """
        self.session_stats['pages_processed'] += 1
        
        # Filtrar apenas vagas novas
        new_jobs = []
        for job in page_jobs:
            if not self.is_job_processed(job):
                new_jobs.append(job)
                self.mark_job_processed(job)
            else:
                self.session_stats['jobs_skipped'] += 1
        
        # Atualizar √∫ltima p√°gina processada
        self.checkpoint_data['last_successful_page'] = page_number
        
        print(f"üìÑ P√°gina {page_number}: {len(new_jobs)} novas / {len(page_jobs)} total")
        
        return new_jobs
    
    def _estimate_time_saved(self) -> int:
        """
        Estima tempo economizado pelo processamento incremental
        
        Baseado em m√©dias hist√≥ricas e estat√≠sticas da sess√£o
        """
        # Estimar baseado em 3 segundos por vaga (busca + processamento)
        time_per_job = 3
        
        # Tempo economizado = vagas puladas * tempo m√©dio
        return self.session_stats['jobs_skipped'] * time_per_job
    
    def start_session(self) -> None:
        """
        Inicia uma nova sess√£o de processamento
        """
        self.session_start = datetime.now()
        self.checkpoint_data['last_run'] = self.session_start.isoformat()
        
        # Resetar estat√≠sticas da sess√£o
        self.session_stats = {
            'jobs_processed': 0,
            'jobs_skipped': 0,
            'pages_processed': 0,
            'pages_skipped': 0,
            'time_saved_seconds': 0
        }
        
        print(f"üöÄ Sess√£o incremental iniciada - {len(self.checkpoint_data['processed_job_ids'])} vagas no hist√≥rico")
    
    def end_session(self) -> None:
        """
        Finaliza sess√£o e salva dados
        """
        session_duration = (datetime.now() - self.session_start).total_seconds()
        
        # Atualizar estat√≠sticas globais
        self.stats_data['total_runs'] += 1
        self.stats_data['total_time_saved_minutes'] += self.session_stats['time_saved_seconds'] / 60
        
        # Calcular m√©dias
        self.stats_data['average_jobs_per_run'] = (
            (self.stats_data['average_jobs_per_run'] * (self.stats_data['total_runs'] - 1) + 
             self.session_stats['jobs_processed']) / self.stats_data['total_runs']
        )
        
        self.stats_data['average_time_saved_per_run'] = (
            self.stats_data['total_time_saved_minutes'] / self.stats_data['total_runs']
        )
        
        # Adicionar ao hist√≥rico
        session_summary = {
            'date': datetime.now().isoformat(),
            'duration_seconds': session_duration,
            'jobs_processed': self.session_stats['jobs_processed'],
            'jobs_skipped': self.session_stats['jobs_skipped'],
            'pages_processed': self.session_stats['pages_processed'],
            'time_saved_seconds': self.session_stats['time_saved_seconds']
        }
        
        self.stats_data['history'].append(session_summary)
        
        # Manter apenas √∫ltimas 100 execu√ß√µes no hist√≥rico
        if len(self.stats_data['history']) > 100:
            self.stats_data['history'] = self.stats_data['history'][-100:]
        
        # Salvar tudo
        self._save_checkpoint()
        self._save_stats()
        
        # Exibir resumo
        self.print_session_summary()
    
    def print_session_summary(self) -> None:
        """
        Exibe resumo da sess√£o de processamento
        """
        print("\nüìä RESUMO DO PROCESSAMENTO INCREMENTAL")
        print("=" * 50)
        print(f"‚úÖ Vagas processadas: {self.session_stats['jobs_processed']}")
        print(f"‚è≠Ô∏è  Vagas puladas: {self.session_stats['jobs_skipped']}")
        print(f"üìÑ P√°ginas processadas: {self.session_stats['pages_processed']}")
        print(f"‚è±Ô∏è  Tempo economizado: {self.session_stats['time_saved_seconds'] // 60}min {self.session_stats['time_saved_seconds'] % 60}s")
        
        if self.session_stats['jobs_processed'] + self.session_stats['jobs_skipped'] > 0:
            efficiency = (self.session_stats['jobs_skipped'] / 
                         (self.session_stats['jobs_processed'] + self.session_stats['jobs_skipped'])) * 100
            print(f"üìà Efici√™ncia: {efficiency:.1f}% das vagas j√° eram conhecidas")
        
        print("=" * 50)
        print(f"üíæ Total no hist√≥rico: {len(self.checkpoint_data['processed_job_ids'])} vagas")
        print(f"‚è∞ Tempo total economizado: {self.stats_data['total_time_saved_minutes']:.1f} minutos")
        print("=" * 50)
    
    def reset_checkpoint(self, confirm: bool = False) -> None:
        """
        Reseta o checkpoint (√∫til para reprocessar tudo)
        
        Args:
            confirm: Confirma√ß√£o de seguran√ßa
        """
        if not confirm:
            print("‚ö†Ô∏è  Para resetar o checkpoint, chame reset_checkpoint(confirm=True)")
            return
        
        self.checkpoint_data = {
            'last_run': None,
            'processed_job_ids': set(),
            'last_successful_page': 0,
            'total_jobs_processed': 0,
            'version': '1.0'
        }
        
        self._save_checkpoint()
        print("üîÑ Checkpoint resetado - pr√≥xima execu√ß√£o processar√° todas as vagas")
    
    def get_stats_report(self) -> Dict:
        """
        Retorna relat√≥rio detalhado de estat√≠sticas
        """
        return {
            'total_runs': self.stats_data['total_runs'],
            'total_jobs_in_history': len(self.checkpoint_data['processed_job_ids']),
            'total_time_saved_minutes': self.stats_data['total_time_saved_minutes'],
            'average_jobs_per_run': self.stats_data['average_jobs_per_run'],
            'average_time_saved_per_run': self.stats_data['average_time_saved_per_run'],
            'last_run': self.checkpoint_data.get('last_run'),
            'recent_runs': self.stats_data['history'][-5:] if self.stats_data['history'] else []
        }