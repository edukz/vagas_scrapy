"""
Sistema de Processamento Incremental

Este mÃ³dulo implementa processamento incremental para o scraper,
permitindo processar apenas vagas novas e economizando atÃ© 90% do tempo
em execuÃ§Ãµes subsequentes.

Funcionalidades:
- ğŸ¯ Detecta vagas jÃ¡ processadas e para quando as encontra
- ğŸ’¾ MantÃ©m checkpoint do Ãºltimo processamento
- ğŸ“Š EstatÃ­sticas de economia de tempo
- ğŸ”„ RecuperaÃ§Ã£o automÃ¡tica em caso de falha
"""

import json
import os
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Set, Optional, Tuple
from pathlib import Path


class IncrementalProcessor:
    """
    Processador incremental para scraping eficiente
    
    MantÃ©m registro das vagas jÃ¡ processadas e permite
    processar apenas novas vagas em execuÃ§Ãµes subsequentes.
    """
    
    def __init__(self, checkpoint_dir: str = "data/checkpoints"):
        """
        Inicializa processador incremental
        
        Args:
            checkpoint_dir: DiretÃ³rio para armazenar checkpoints
        """
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_file = os.path.join(checkpoint_dir, "incremental_checkpoint.json")
        self.stats_file = os.path.join(checkpoint_dir, "incremental_stats.json")
        
        # Criar diretÃ³rio se nÃ£o existir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # Carregar checkpoint e estatÃ­sticas
        self.checkpoint_data = self._load_checkpoint()
        self.stats_data = self._load_stats()
        
        # Estado da sessÃ£o atual
        self.session_start = datetime.now()
        self.session_stats = {
            'jobs_processed': 0,
            'jobs_skipped': 0,
            'pages_processed': 0,
            'pages_skipped': 0,
            'time_saved_seconds': 0
        }
    
    def _generate_job_id(self, job_data: Dict) -> str:
        """
        Gera ID Ãºnico para uma vaga baseado em seus dados
        
        Usa uma combinaÃ§Ã£o de tÃ­tulo, empresa e link para criar
        um identificador Ãºnico e estÃ¡vel.
        """
        # Campos usados para gerar o ID
        id_components = [
            job_data.get('titulo', ''),
            job_data.get('empresa', ''),
            job_data.get('link', ''),
            job_data.get('localizacao', '')
        ]
        
        # Criar string Ãºnica
        id_string = '|'.join(str(comp) for comp in id_components)
        
        # Gerar hash MD5
        return hashlib.md5(id_string.encode()).hexdigest()
    
    def _load_checkpoint(self) -> Dict:
        """
        Carrega checkpoint do Ãºltimo processamento
        """
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # Converter lista de IDs para set para busca rÃ¡pida
                if 'processed_job_ids' in data:
                    data['processed_job_ids'] = set(data['processed_job_ids'])
                
                print(f"âœ… Checkpoint carregado: {len(data.get('processed_job_ids', []))} vagas conhecidas")
                return data
                
            except Exception as e:
                print(f"âš ï¸ Erro ao carregar checkpoint: {e}")
        
        # Checkpoint padrÃ£o
        return {
            'last_run': None,
            'processed_job_ids': set(),
            'last_successful_page': 0,
            'total_jobs_processed': 0,
            'version': '1.0'
        }
    
    def _load_stats(self) -> Dict:
        """
        Carrega estatÃ­sticas histÃ³ricas
        """
        if os.path.exists(self.stats_file):
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        
        # EstatÃ­sticas padrÃ£o
        return {
            'total_runs': 0,
            'total_time_saved_minutes': 0,
            'average_jobs_per_run': 0,
            'average_time_saved_per_run': 0,
            'history': []
        }
    
    def _save_checkpoint(self) -> None:
        """
        Salva checkpoint atual
        """
        try:
            # Preparar dados para serializaÃ§Ã£o
            checkpoint_data = self.checkpoint_data.copy()
            
            # Converter set para lista
            if 'processed_job_ids' in checkpoint_data:
                checkpoint_data['processed_job_ids'] = list(checkpoint_data['processed_job_ids'])
            
            # Salvar
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
            print(f"ğŸ’¾ Checkpoint salvo: {len(self.checkpoint_data['processed_job_ids'])} vagas registradas")
            
        except Exception as e:
            print(f"âŒ Erro ao salvar checkpoint: {e}")
    
    def _save_stats(self) -> None:
        """
        Salva estatÃ­sticas atualizadas
        """
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ Erro ao salvar estatÃ­sticas: {e}")
    
    def is_job_processed(self, job_data: Dict) -> bool:
        """
        Verifica se uma vaga jÃ¡ foi processada anteriormente
        """
        job_id = self._generate_job_id(job_data)
        return job_id in self.checkpoint_data['processed_job_ids']
    
    def mark_job_processed(self, job_data: Dict) -> None:
        """
        Marca uma vaga como processada
        """
        job_id = self._generate_job_id(job_data)
        self.checkpoint_data['processed_job_ids'].add(job_id)
        self.checkpoint_data['total_jobs_processed'] += 1
        self.session_stats['jobs_processed'] += 1
    
    def should_continue_processing(self, current_page_jobs: List[Dict], 
                                  threshold: float = 0.5) -> Tuple[bool, List[Dict]]:
        """
        Determina se deve continuar processando baseado em vagas jÃ¡ conhecidas
        
        Args:
            current_page_jobs: Lista de vagas da pÃ¡gina atual
            threshold: Percentual de vagas novas necessÃ¡rio para continuar (0.5 = 50%)
            
        Returns:
            (should_continue, new_jobs): Se deve continuar e lista de vagas novas
        """
        if not current_page_jobs:
            return True, []
        
        new_jobs = []
        known_jobs = 0
        
        for job in current_page_jobs:
            if self.is_job_processed(job):
                known_jobs += 1
                self.session_stats['jobs_skipped'] += 1
            else:
                new_jobs.append(job)
        
        # Calcular proporÃ§Ã£o de vagas novas
        new_ratio = len(new_jobs) / len(current_page_jobs)
        
        # DecisÃ£o de continuar
        should_continue = new_ratio >= threshold
        
        if not should_continue:
            print(f"ğŸ›‘ Parando processamento: {known_jobs}/{len(current_page_jobs)} vagas jÃ¡ conhecidas")
            self.session_stats['time_saved_seconds'] = self._estimate_time_saved()
        else:
            print(f"âœ… Continuando: {len(new_jobs)} vagas novas encontradas")
        
        return should_continue, new_jobs
    
    def process_page_incrementally(self, page_jobs: List[Dict], page_number: int) -> List[Dict]:
        """
        Processa uma pÃ¡gina de forma incremental
        
        Returns:
            Lista de vagas novas (nÃ£o processadas anteriormente)
        """
        self.session_stats['pages_processed'] += 1
        
        # Filtrar apenas vagas novas
        new_jobs = []
        for job in page_jobs:
            if not self.is_job_processed(job):
                new_jobs.append(job)
                self.mark_job_processed(job)
            else:
                self.session_stats['jobs_skipped'] += 1
        
        # Atualizar Ãºltima pÃ¡gina processada
        self.checkpoint_data['last_successful_page'] = page_number
        
        print(f"ğŸ“„ PÃ¡gina {page_number}: {len(new_jobs)} novas / {len(page_jobs)} total")
        
        return new_jobs
    
    def _estimate_time_saved(self) -> int:
        """
        Estima tempo economizado pelo processamento incremental
        
        Baseado em mÃ©dias histÃ³ricas e estatÃ­sticas da sessÃ£o
        """
        # Estimar baseado em 3 segundos por vaga (busca + processamento)
        time_per_job = 3
        
        # Tempo economizado = vagas puladas * tempo mÃ©dio
        return self.session_stats['jobs_skipped'] * time_per_job
    
    def start_session(self) -> None:
        """
        Inicia uma nova sessÃ£o de processamento
        """
        self.session_start = datetime.now()
        self.checkpoint_data['last_run'] = self.session_start.isoformat()
        
        # Resetar estatÃ­sticas da sessÃ£o
        self.session_stats = {
            'jobs_processed': 0,
            'jobs_skipped': 0,
            'pages_processed': 0,
            'pages_skipped': 0,
            'time_saved_seconds': 0
        }
        
        print(f"ğŸš€ SessÃ£o incremental iniciada - {len(self.checkpoint_data['processed_job_ids'])} vagas no histÃ³rico")
    
    def end_session(self) -> None:
        """
        Finaliza sessÃ£o e salva dados
        """
        session_duration = (datetime.now() - self.session_start).total_seconds()
        
        # Atualizar estatÃ­sticas globais
        self.stats_data['total_runs'] += 1
        self.stats_data['total_time_saved_minutes'] += self.session_stats['time_saved_seconds'] / 60
        
        # Calcular mÃ©dias
        self.stats_data['average_jobs_per_run'] = (
            (self.stats_data['average_jobs_per_run'] * (self.stats_data['total_runs'] - 1) + 
             self.session_stats['jobs_processed']) / self.stats_data['total_runs']
        )
        
        self.stats_data['average_time_saved_per_run'] = (
            self.stats_data['total_time_saved_minutes'] / self.stats_data['total_runs']
        )
        
        # Adicionar ao histÃ³rico
        session_summary = {
            'date': datetime.now().isoformat(),
            'duration_seconds': session_duration,
            'jobs_processed': self.session_stats['jobs_processed'],
            'jobs_skipped': self.session_stats['jobs_skipped'],
            'pages_processed': self.session_stats['pages_processed'],
            'time_saved_seconds': self.session_stats['time_saved_seconds']
        }
        
        self.stats_data['history'].append(session_summary)
        
        # Manter apenas Ãºltimas 100 execuÃ§Ãµes no histÃ³rico
        if len(self.stats_data['history']) > 100:
            self.stats_data['history'] = self.stats_data['history'][-100:]
        
        # Salvar tudo
        self._save_checkpoint()
        self._save_stats()
        
        # Exibir resumo
        self.print_session_summary()
    
    def print_session_summary(self) -> None:
        """
        Exibe resumo da sessÃ£o de processamento
        """
        print("\nğŸ“Š RESUMO DO PROCESSAMENTO INCREMENTAL")
        print("=" * 50)
        print(f"âœ… Vagas processadas: {self.session_stats['jobs_processed']}")
        print(f"â­ï¸  Vagas puladas: {self.session_stats['jobs_skipped']}")
        print(f"ğŸ“„ PÃ¡ginas processadas: {self.session_stats['pages_processed']}")
        print(f"â±ï¸  Tempo economizado: {self.session_stats['time_saved_seconds'] // 60}min {self.session_stats['time_saved_seconds'] % 60}s")
        
        if self.session_stats['jobs_processed'] + self.session_stats['jobs_skipped'] > 0:
            efficiency = (self.session_stats['jobs_skipped'] / 
                         (self.session_stats['jobs_processed'] + self.session_stats['jobs_skipped'])) * 100
            print(f"ğŸ“ˆ EficiÃªncia: {efficiency:.1f}% das vagas jÃ¡ eram conhecidas")
        
        print("=" * 50)
        print(f"ğŸ’¾ Total no histÃ³rico: {len(self.checkpoint_data['processed_job_ids'])} vagas")
        print(f"â° Tempo total economizado: {self.stats_data['total_time_saved_minutes']:.1f} minutos")
        print("=" * 50)
    
    def reset_checkpoint(self, confirm: bool = False) -> None:
        """
        Reseta o checkpoint (Ãºtil para reprocessar tudo)
        
        Args:
            confirm: ConfirmaÃ§Ã£o de seguranÃ§a
        """
        if not confirm:
            print("âš ï¸  Para resetar o checkpoint, chame reset_checkpoint(confirm=True)")
            return
        
        self.checkpoint_data = {
            'last_run': None,
            'processed_job_ids': set(),
            'last_successful_page': 0,
            'total_jobs_processed': 0,
            'version': '1.0'
        }
        
        self._save_checkpoint()
        print("ğŸ”„ Checkpoint resetado - prÃ³xima execuÃ§Ã£o processarÃ¡ todas as vagas")
    
    def get_stats_report(self) -> Dict:
        """
        Retorna relatÃ³rio detalhado de estatÃ­sticas
        """
        return {
            'total_runs': self.stats_data['total_runs'],
            'total_jobs_in_history': len(self.checkpoint_data['processed_job_ids']),
            'total_time_saved_minutes': self.stats_data['total_time_saved_minutes'],
            'average_jobs_per_run': self.stats_data['average_jobs_per_run'],
            'average_time_saved_per_run': self.stats_data['average_time_saved_per_run'],
            'last_run': self.checkpoint_data.get('last_run'),
            'recent_runs': self.stats_data['history'][-5:] if self.stats_data['history'] else []
        }