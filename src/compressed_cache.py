"""
Sistema de Cache Comprimido

Este m√≥dulo estende o cache inteligente adicionando compress√£o gzip
para reduzir o uso de disco em 60-80%.

Benef√≠cios:
- üíæ Economia significativa de espa√ßo
- üöÄ Leitura mais r√°pida para arquivos grandes
- üîí Compatibilidade total com o cache existente
"""

import json
import os
import gzip
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Optional, List
from pathlib import Path

from .cache import CacheEntry, IntelligentCache
from .cache_index import CacheIndex


class CompressedCache(IntelligentCache):
    """
    Sistema de cache com compress√£o autom√°tica
    
    Reduz o tamanho dos arquivos de cache em 60-80% usando gzip,
    mantendo compatibilidade total com a interface existente.
    """
    
    def __init__(self, cache_dir: str = "data/cache", max_age_hours: int = 6, compression_level: int = 6):
        """
        Inicializa cache comprimido
        
        Args:
            cache_dir: Diret√≥rio para armazenar cache
            max_age_hours: Tempo de vida do cache em horas
            compression_level: N√≠vel de compress√£o gzip (1-9, default 6)
        """
        self.compression_level = compression_level
        super().__init__(cache_dir, max_age_hours)
        
        # Sistema de √≠ndices para busca r√°pida
        self.index = CacheIndex(cache_dir)
        
        # Estat√≠sticas de compress√£o
        self.compression_stats = {
            'total_saved_bytes': 0,
            'total_files_compressed': 0,
            'average_compression_ratio': 0.0
        }
        
        # Migrar cache existente para formato comprimido
        self._migrate_existing_cache()
    
    def _get_cache_file_path(self, cache_key: str) -> str:
        """Gera caminho do arquivo de cache comprimido"""
        return os.path.join(self.cache_dir, f"{cache_key}.json.gz")
    
    def _get_legacy_cache_path(self, cache_key: str) -> str:
        """Gera caminho do arquivo de cache antigo (sem compress√£o)"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    async def get(self, url: str) -> Optional[Dict]:
        """
        Recupera dados do cache se v√°lidos
        
        Tenta primeiro o formato comprimido, depois o formato legado
        """
        cache_key = self._get_cache_key(url)
        
        # Verificar cache em mem√≥ria primeiro
        if cache_key in self.memory_cache:
            entry = self.memory_cache[cache_key]
            if not entry.is_expired(self.max_age_hours):
                print(f"‚úì Cache hit (mem√≥ria): {url[:50]}...")
                return entry.data
            else:
                del self.memory_cache[cache_key]
        
        # Verificar cache comprimido em disco
        cache_file = self._get_cache_file_path(cache_key)
        if os.path.exists(cache_file):
            try:
                # Ler e descomprimir
                with gzip.open(cache_file, 'rt', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                entry = CacheEntry.from_dict(cache_data)
                
                if not entry.is_expired(self.max_age_hours):
                    # Carregar de volta para mem√≥ria
                    self.memory_cache[cache_key] = entry
                    print(f"‚úì Cache hit (disco comprimido): {url[:50]}...")
                    return entry.data
                else:
                    # Cache expirado, remover arquivo
                    os.remove(cache_file)
            except Exception as e:
                print(f"‚ö† Erro ao ler cache comprimido: {e}")
        
        # Verificar cache legado (n√£o comprimido)
        legacy_file = self._get_legacy_cache_path(cache_key)
        if os.path.exists(legacy_file):
            try:
                with open(legacy_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                entry = CacheEntry.from_dict(cache_data)
                
                if not entry.is_expired(self.max_age_hours):
                    # Migrar para formato comprimido
                    await self.set(url, entry.data)
                    
                    # Remover arquivo legado
                    os.remove(legacy_file)
                    
                    print(f"‚úì Cache hit (disco legado, migrado): {url[:50]}...")
                    return entry.data
                else:
                    # Cache expirado, remover arquivo
                    os.remove(legacy_file)
            except Exception as e:
                print(f"‚ö† Erro ao ler cache legado: {e}")
        
        return None
    
    async def set(self, url: str, data: Dict) -> None:
        """
        Armazena dados no cache com compress√£o e indexa√ß√£o autom√°tica
        """
        cache_key = self._get_cache_key(url)
        entry = CacheEntry(
            data=data,
            timestamp=datetime.now(),
            url=url,
            hash_key=cache_key
        )
        
        # Armazenar em mem√≥ria
        self.memory_cache[cache_key] = entry
        
        # Armazenar em disco com compress√£o
        cache_file = self._get_cache_file_path(cache_key)
        try:
            # Serializar para JSON
            json_str = json.dumps(entry.to_dict(), ensure_ascii=False, indent=2)
            json_bytes = json_str.encode('utf-8')
            
            # Comprimir e salvar
            with gzip.open(cache_file, 'wt', encoding='utf-8', compresslevel=self.compression_level) as f:
                json.dump(entry.to_dict(), f, ensure_ascii=False, indent=2)
            
            # Calcular estat√≠sticas
            compressed_size = os.path.getsize(cache_file)
            original_size = len(json_bytes)
            saved_bytes = original_size - compressed_size
            compression_ratio = (saved_bytes / original_size) * 100
            
            # Atualizar estat√≠sticas
            self.compression_stats['total_saved_bytes'] += saved_bytes
            self.compression_stats['total_files_compressed'] += 1
            self.compression_stats['average_compression_ratio'] = (
                (self.compression_stats['average_compression_ratio'] * 
                 (self.compression_stats['total_files_compressed'] - 1) + 
                 compression_ratio) / self.compression_stats['total_files_compressed']
            )
            
            # Indexar automaticamente se dados cont√™m jobs
            if 'jobs' in data and isinstance(data['jobs'], list):
                self.index.add_entry(
                    cache_key=cache_key,
                    file_path=cache_file,
                    url=url,
                    jobs_data=data['jobs'],
                    original_size=original_size,
                    compressed_size=compressed_size
                )
                print(f"‚úì Cache salvo e indexado (comprimido {compression_ratio:.1f}%): {url[:50]}...")
            else:
                print(f"‚úì Cache salvo (comprimido {compression_ratio:.1f}%): {url[:50]}...")
            
        except Exception as e:
            print(f"‚ö† Erro ao salvar cache comprimido: {e}")
    
    def _migrate_existing_cache(self) -> None:
        """
        Migra arquivos de cache existentes para formato comprimido
        """
        migrated = 0
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json') and not filename.endswith('.json.gz'):
                    legacy_file = os.path.join(self.cache_dir, filename)
                    
                    try:
                        # Ler arquivo antigo
                        with open(legacy_file, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        entry = CacheEntry.from_dict(cache_data)
                        
                        # Se n√£o expirado, comprimir
                        if not entry.is_expired(self.max_age_hours):
                            cache_key = filename.replace('.json', '')
                            compressed_file = self._get_cache_file_path(cache_key)
                            
                            # Salvar comprimido
                            with gzip.open(compressed_file, 'wt', encoding='utf-8', 
                                         compresslevel=self.compression_level) as f:
                                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                            
                            migrated += 1
                        
                        # Remover arquivo antigo
                        os.remove(legacy_file)
                        
                    except Exception as e:
                        print(f"‚ö† Erro ao migrar {filename}: {e}")
            
            if migrated > 0:
                print(f"‚úÖ {migrated} arquivos de cache migrados para formato comprimido")
                
        except Exception as e:
            print(f"‚ö† Erro na migra√ß√£o do cache: {e}")
    
    async def _cleanup_expired_cache(self) -> None:
        """
        Remove entradas de cache expiradas (comprimidas e legadas)
        """
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json.gz') or (filename.endswith('.json') and not filename.endswith('.json.gz')):
                    cache_file = os.path.join(self.cache_dir, filename)
                    try:
                        # Determinar se √© comprimido ou n√£o
                        if filename.endswith('.json.gz'):
                            with gzip.open(cache_file, 'rt', encoding='utf-8') as f:
                                cache_data = json.load(f)
                        else:
                            with open(cache_file, 'r', encoding='utf-8') as f:
                                cache_data = json.load(f)
                        
                        entry = CacheEntry.from_dict(cache_data)
                        if entry.is_expired(self.max_age_hours):
                            os.remove(cache_file)
                            print(f"üóëÔ∏è Cache expirado removido: {filename}")
                    except:
                        # Se houver erro ao ler, remover arquivo corrompido
                        os.remove(cache_file)
        except Exception as e:
            print(f"‚ö† Erro na limpeza do cache: {e}")
    
    def get_compression_stats(self) -> Dict:
        """
        Retorna estat√≠sticas de compress√£o
        """
        stats = self.compression_stats.copy()
        
        # Adicionar estat√≠sticas do diret√≥rio
        total_size = 0
        compressed_files = 0
        legacy_files = 0
        
        try:
            for filename in os.listdir(self.cache_dir):
                file_path = os.path.join(self.cache_dir, filename)
                if filename.endswith('.json.gz'):
                    compressed_files += 1
                    total_size += os.path.getsize(file_path)
                elif filename.endswith('.json'):
                    legacy_files += 1
                    total_size += os.path.getsize(file_path)
        except:
            pass
        
        stats.update({
            'total_cache_size_mb': total_size / (1024 * 1024),
            'compressed_files': compressed_files,
            'legacy_files': legacy_files,
            'estimated_savings_mb': stats['total_saved_bytes'] / (1024 * 1024)
        })
        
        return stats
    
    def search_cache(self, criteria: Dict) -> List:
        """
        Busca entries no cache usando crit√©rios
        
        Args:
            criteria: Crit√©rios de busca (ver CacheIndex.search)
            
        Returns:
            Lista de CacheIndexEntry que atendem aos crit√©rios
        """
        return self.index.search(criteria)
    
    def get_cache_stats(self) -> Dict:
        """
        Retorna estat√≠sticas combinadas do cache e √≠ndices
        """
        compression_stats = self.get_compression_stats()
        index_stats = self.index.get_stats()
        
        return {
            'compression': compression_stats,
            'index': index_stats
        }
    
    def get_top_companies(self, limit: int = 10) -> List:
        """
        Retorna empresas com mais vagas no cache
        """
        return self.index.get_top_companies(limit)
    
    def get_top_technologies(self, limit: int = 10) -> List:
        """
        Retorna tecnologias mais demandadas no cache
        """
        return self.index.get_top_technologies(limit)
    
    def get_recent_entries(self, days: int = 7) -> List:
        """
        Retorna entries dos √∫ltimos N dias
        """
        return self.index.get_entries_by_date_range(days)
    
    def rebuild_index(self) -> int:
        """
        Reconstr√≥i o √≠ndice completo do cache
        """
        return self.index.rebuild_index(self.cache_dir)
    
    def print_compression_report(self) -> None:
        """
        Exibe relat√≥rio completo de compress√£o e √≠ndices
        """
        compression_stats = self.get_compression_stats()
        index_stats = self.index.get_stats()
        
        print("\nüìä RELAT√ìRIO DO CACHE COMPRIMIDO + √çNDICES")
        print("=" * 60)
        
        # Estat√≠sticas de compress√£o
        print("üóúÔ∏è COMPRESS√ÉO:")
        print(f"  üìÅ Arquivos comprimidos: {compression_stats['compressed_files']}")
        print(f"  üìÑ Arquivos legados: {compression_stats['legacy_files']}")
        print(f"  üíæ Tamanho total: {compression_stats['total_cache_size_mb']:.2f} MB")
        print(f"  üìà Taxa m√©dia de compress√£o: {compression_stats['average_compression_ratio']:.1f}%")
        print(f"  üí∞ Economia: {compression_stats['estimated_savings_mb']:.2f} MB")
        
        # Estat√≠sticas do √≠ndice
        print(f"\nüîç √çNDICES:")
        print(f"  üìã Entradas indexadas: {index_stats['total_entries']}")
        print(f"  üíº Total de vagas: {index_stats['total_jobs']}")
        print(f"  üìÖ √öltima atualiza√ß√£o: {index_stats.get('last_updated', 'N/A')[:19]}")
        
        # Top empresas e tecnologias
        top_companies = self.get_top_companies(3)
        if top_companies:
            print(f"\nüè¢ TOP EMPRESAS:")
            for company, count in top_companies:
                print(f"  ‚Ä¢ {company}: {count} vagas")
        
        top_techs = self.get_top_technologies(3)
        if top_techs:
            print(f"\nüíª TOP TECNOLOGIAS:")
            for tech, count in top_techs:
                print(f"  ‚Ä¢ {tech}: {count} vagas")
        
        print("=" * 60)