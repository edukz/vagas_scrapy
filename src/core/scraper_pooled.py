"""
Scraper com Pool de ConexÃµes

Esta versÃ£o integra o pool de conexÃµes reutilizÃ¡veis para mÃ¡xima performance,
reduzindo o overhead de criaÃ§Ã£o/destruiÃ§Ã£o de pÃ¡ginas.

BenefÃ­cios:
- âš¡ 100-500ms mais rÃ¡pido por requisiÃ§Ã£o
- ðŸ”„ ReutilizaÃ§Ã£o inteligente de recursos
- ðŸ“Š MÃ©tricas detalhadas de performance
"""

import asyncio
import re
import time
from typing import List, Dict, Optional, Tuple
from playwright.async_api import async_playwright, Page

# Importar sistemas otimizados
from ..systems.compressed_cache import CompressedCache
from ..systems.incremental_processor import IncrementalProcessor
from ..systems.connection_pool import ConnectionPool, get_pooled_page, return_pooled_page, connection_pool
from ..systems.deduplicator import JobDeduplicator

# Importar sistemas existentes
from ..utils.utils import RateLimiter, PerformanceMonitor
from ..systems.navigation import PageNavigatorFixed as PageNavigator
from ..systems.retry_system import RetrySystem, STRATEGIES
from ..systems.selector_fallback import fallback_selector
from ..systems.data_validator import data_validator
from ..systems.structured_logger import structured_logger, Component, LogLevel
from ..systems.circuit_breaker import circuit_breaker_manager, CIRCUIT_CONFIGS, CircuitBreakerError
from ..systems.metrics_tracker import metrics_tracker, setup_default_alerts, TimerContext
from ..systems.alert_system import alert_system, setup_default_alert_rules, integrate_with_metrics


class PooledPageManager:
    """
    Gerenciador de pÃ¡ginas com pool de conexÃµes
    
    Context manager que obtÃ©m pÃ¡gina do pool e a retorna automaticamente
    """
    
    def __init__(self, timeout_seconds: float = 10.0):
        self.timeout_seconds = timeout_seconds
        self.page: Optional[Page] = None
        self.had_error = False
    
    async def __aenter__(self) -> Page:
        """ObtÃ©m pÃ¡gina do pool"""
        self.page = await get_pooled_page(self.timeout_seconds)
        if not self.page:
            raise RuntimeError("NÃ£o foi possÃ­vel obter pÃ¡gina do pool")
        return self.page
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Retorna pÃ¡gina ao pool"""
        if self.page:
            # Marcar erro se houve exceÃ§Ã£o
            self.had_error = exc_type is not None
            await return_pooled_page(self.page, self.had_error)
    
    def mark_error(self):
        """Marca que houve erro (para casos manuais)"""
        self.had_error = True


async def scrape_catho_jobs_pooled(
    max_concurrent_jobs: int = 3, 
    max_pages: int = 5,
    incremental: bool = True,
    show_compression_stats: bool = True,
    show_pool_stats: bool = True,
    pool_min_size: int = 2,
    pool_max_size: int = 8,
    enable_deduplication: bool = True
) -> List[Dict]:
    """
    FunÃ§Ã£o principal de scraping com pool de conexÃµes
    
    Args:
        max_concurrent_jobs: NÃºmero mÃ¡ximo de vagas processadas simultaneamente
        max_pages: NÃºmero mÃ¡ximo de pÃ¡ginas para processar
        incremental: Se True, processa apenas vagas novas
        show_compression_stats: Se True, exibe estatÃ­sticas de compressÃ£o
        show_pool_stats: Se True, exibe estatÃ­sticas do pool
        pool_min_size: Tamanho mÃ­nimo do pool de conexÃµes
        pool_max_size: Tamanho mÃ¡ximo do pool de conexÃµes
        
    Returns:
        Lista de vagas encontradas (apenas novas se incremental=True)
    """
    base_url = "https://www.catho.com.br/vagas/home-office/"
    
    # Inicializar sistemas otimizados
    cache = CompressedCache()
    incremental_processor = IncrementalProcessor() if incremental else None
    deduplicator = JobDeduplicator() if enable_deduplication else None
    
    # Configurar pool global
    connection_pool.min_size = pool_min_size
    connection_pool.max_size = pool_max_size
    connection_pool.max_age_minutes = 30
    connection_pool.cleanup_interval = 60
    
    # Inicializar sistemas existentes
    rate_limiter = RateLimiter(requests_per_second=2.0, burst_limit=5, adaptive=True)
    performance_monitor = PerformanceMonitor()
    navigator = PageNavigator(max_pages=max_pages)
    retry_system = RetrySystem(default_strategy=STRATEGIES['standard'])
    
    print("ðŸ“Š Monitoramento de performance iniciado")
    print("ðŸ›¡ï¸ Sistema de retry ativado para maior robustez")
    print("ðŸ”§ Circuit Breakers configurados para proteÃ§Ã£o automÃ¡tica")
    print("ðŸ“Š Sistema de mÃ©tricas ativado para monitoramento em tempo real")
    print("ðŸ—œï¸ Cache comprimido ativado para economia de espaÃ§o")
    print("ðŸ”„ Pool de conexÃµes ativado para mÃ¡xima performance")
    
    if incremental:
        print("âš¡ Processamento incremental ativado - apenas vagas novas serÃ£o processadas")
        incremental_processor.start_session()
    
    if enable_deduplication:
        print("ðŸ” Sistema de deduplicaÃ§Ã£o ativado - duplicatas serÃ£o removidas")
    
    # Configurar alertas e mÃ©tricas
    setup_default_alerts()
    setup_default_alert_rules()
    integrate_with_metrics()
    alert_system.start_background_monitoring()
    
    print("ðŸš¨ Sistema de alertas automÃ¡ticos configurado e ativo")
    
    try:
        with TimerContext("scraper.total_duration"):
            async with async_playwright() as p:
                print("ðŸš€ Iniciando navegador otimizado com pool de conexÃµes...")
                browser = await p.chromium.launch(
                    headless=False,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--no-sandbox',
                        '--disable-background-timer-throttling',
                        '--disable-backgrounding-occluded-windows',
                        '--disable-renderer-backgrounding'
                    ]
                )
                
                # Inicializar pool de conexÃµes
                await connection_pool.initialize(browser)
                
                try:
                    print(f"ðŸŒ Iniciando coleta de mÃºltiplas pÃ¡ginas (mÃ¡x: {max_pages} pÃ¡ginas)")
                    
                    all_jobs = []
                    seen_urls = set()
                    current_page = 1
                    should_continue = True
                    
                    # Primeira pÃ¡gina com pool
                    print(f"\nðŸ“„ === PÃGINA {current_page} ===")
                    
                    async with PooledPageManager() as page:
                        await page.goto(base_url, wait_until='networkidle', timeout=60000)
                        await page.wait_for_timeout(3000)
                        
                        title = await page.title()
                        print(f"TÃ­tulo da pÃ¡gina: {title}")
                        
                        # Detectar tipo de paginaÃ§Ã£o
                        pagination_type = await navigator.detect_pagination_type(page)
                        print(f"ðŸ” Tipo de paginaÃ§Ã£o detectado: {pagination_type}")
                        
                        # Processar primeira pÃ¡gina
                        try:
                            page_jobs = await extract_jobs_from_current_page_pooled(
                                page, seen_urls, retry_system, cache
                            )
                            
                            # Processamento incremental
                            if incremental_processor:
                                should_continue, new_jobs = incremental_processor.should_continue_processing(
                                    page_jobs, threshold=0.3
                                )
                                
                                if should_continue:
                                    page_jobs = incremental_processor.process_page_incrementally(
                                        page_jobs, current_page
                                    )
                            
                            all_jobs.extend(page_jobs)
                            print(f"âœ… PÃ¡gina {current_page}: {len(page_jobs)} vagas {'novas ' if incremental else ''}coletadas")
                            
                            metrics_tracker.increment_counter("scraper.pages_processed")
                            metrics_tracker.increment_counter("scraper.jobs_found", len(page_jobs))
                            
                        except Exception as e:
                            print(f"ðŸ”´ Erro na pÃ¡gina {current_page}: {e}")
                            metrics_tracker.increment_counter("scraper.pages_failed")
                            return []
                    
                    # Navegar pelas pÃ¡ginas restantes com pool
                    if should_continue and pagination_type != "single_page" and max_pages > 1:
                        if pagination_type == "traditional":
                            # Usar pÃ¡gina temporÃ¡ria para detectar nÃºmeros
                            async with PooledPageManager() as detect_page:
                                await detect_page.goto(base_url, wait_until='networkidle', timeout=60000)
                                page_numbers = await navigator.get_page_numbers(detect_page)
                            
                            if page_numbers:
                                # Usar max_pages do usuÃ¡rio, independente de quantas pÃ¡ginas o site tem
                                print(f"ðŸ“Š PÃ¡ginas disponÃ­veis no site: {max(page_numbers)}")
                                print(f"ðŸ“‹ PÃ¡ginas a processar (configurado): {max_pages}")
                                pages_to_try = list(range(2, max_pages + 1))
                            else:
                                print("âš  NÃºmeros de pÃ¡ginas nÃ£o detectados, tentando navegaÃ§Ã£o forÃ§ada...")
                                pages_to_try = list(range(2, max_pages + 1))
                            
                            # Processar pÃ¡ginas restantes com pool
                            for page_num in pages_to_try:
                                if not should_continue:
                                    break
                                    
                                print(f"\nðŸ“„ === PÃGINA {page_num} ===")
                                
                                try:
                                    async with PooledPageManager() as page:
                                        # NavegaÃ§Ã£o para prÃ³xima pÃ¡gina
                                        success = await navigator.navigate_to_page(page, page_num, base_url)
                                        
                                        if success:
                                            # ExtraÃ§Ã£o da pÃ¡gina
                                            page_jobs = await extract_jobs_from_current_page_pooled(
                                                page, seen_urls, retry_system, cache
                                            )
                                            
                                            # Processamento incremental
                                            if incremental_processor:
                                                should_continue, new_jobs = incremental_processor.should_continue_processing(
                                                    page_jobs, threshold=0.3
                                                )
                                                
                                                if should_continue:
                                                    page_jobs = incremental_processor.process_page_incrementally(
                                                        page_jobs, page_num
                                                    )
                                            
                                            all_jobs.extend(page_jobs)
                                            print(f"âœ… PÃ¡gina {page_num}: {len(page_jobs)} vagas {'novas ' if incremental else ''}coletadas")
                                            
                                            metrics_tracker.increment_counter("scraper.pages_processed")
                                            metrics_tracker.increment_counter("scraper.jobs_found", len(page_jobs))
                                            
                                            # Rate limiting entre pÃ¡ginas
                                            await rate_limiter.acquire()
                                            
                                        else:
                                            print(f"âŒ Falha ao navegar para pÃ¡gina {page_num}")
                                            metrics_tracker.increment_counter("scraper.pages_failed")
                                            break
                                            
                                except Exception as e:
                                    print(f"ðŸ”´ Erro na pÃ¡gina {page_num}: {e}")
                                    metrics_tracker.increment_counter("scraper.pages_failed")
                                    continue
                    
                    print(f"\nâœ… Coleta concluÃ­da! Total: {len(all_jobs)} vagas {'novas ' if incremental else ''}encontradas")
                    
                    # Aplicar deduplicaÃ§Ã£o se habilitada
                    if enable_deduplication and deduplicator and all_jobs:
                        print(f"\nðŸ” Aplicando deduplicaÃ§Ã£o em {len(all_jobs)} vagas...")
                        all_jobs = deduplicator.deduplicate_jobs(all_jobs)
                        print(f"âœ… ApÃ³s deduplicaÃ§Ã£o: {len(all_jobs)} vagas Ãºnicas")
                        deduplicator.print_stats()
                    
                    # Finalizar processamento incremental
                    if incremental_processor:
                        incremental_processor.end_session()
                    
                    # Exibir estatÃ­sticas
                    if show_compression_stats:
                        cache.print_compression_report()
                    
                    if show_pool_stats:
                        connection_pool.print_stats()
                    
                    # Cleanup
                    alert_system.stop_background_monitoring()
                    await connection_pool.shutdown()
                    
                    print("ðŸ”„ Fechando navegador automaticamente...")
                    return all_jobs
                    
                except Exception as e:
                    print(f"Erro durante o scraping: {e}")
                    await connection_pool.shutdown()
                    return []
                
    except Exception as e:
        if "Executable doesn't exist" in str(e):
            print("âŒ ERRO: Navegadores do Playwright nÃ£o encontrados!")
            print("ðŸ“‹ SOLUÃ‡ÃƒO:")
            print("   1. Abra o prompt do Windows (cmd)")
            print("   2. Execute: python -m playwright install")
            print("   3. Aguarde a instalaÃ§Ã£o dos navegadores")
            print("   4. Execute o script novamente")
            print("\nðŸ’¡ Isso sÃ³ precisa ser feito uma vez.")
            return []
        else:
            print(f"âŒ Erro inesperado: {e}")
            return []


async def extract_jobs_from_current_page_pooled(
    page: Page, 
    seen_urls: set, 
    retry_system: RetrySystem = None,
    cache: Optional[CompressedCache] = None
) -> List[Dict]:
    """
    Extrai vagas da pÃ¡gina atual usando pool de conexÃµes
    """
    jobs = []
    
    try:
        # Tentar buscar no cache primeiro
        if cache:
            page_url = page.url
            cached_data = await cache.get(page_url)
            if cached_data:
                print(f"ðŸŽ¯ PÃ¡gina inteira recuperada do cache comprimido!")
                return cached_data.get('jobs', [])
        
        # Se nÃ£o estÃ¡ no cache, fazer extraÃ§Ã£o
        job_elements = await page.query_selector_all('h2 a[href*="/vagas/"]')
        
        for element in job_elements[:10]:  # Limitar para teste
            try:
                link = await element.get_attribute('href')
                title_text = await element.text_content()
                
                if link and link not in seen_urls:
                    seen_urls.add(link)
                    
                    job = {
                        'titulo': title_text or 'TÃ­tulo nÃ£o encontrado',
                        'link': link,
                        'empresa': 'Empresa nÃ£o identificada',
                        'localizacao': 'Home Office',
                        'salario': 'NÃ£o informado',
                        'regime': 'Home Office',
                        'nivel': 'NÃ£o especificado',
                        'tecnologias_detectadas': [],
                        'data_coleta': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    jobs.append(job)
                    
            except Exception as e:
                print(f"Erro ao processar elemento: {e}")
                continue
        
        # Salvar no cache para prÃ³ximas execuÃ§Ãµes
        if cache and jobs:
            await cache.set(page.url, {'jobs': jobs, 'timestamp': time.time()})
    
    except Exception as e:
        print(f"Erro na extraÃ§Ã£o: {e}")
    
    return jobs


async def benchmark_pool_performance(
    pages_to_test: int = 5,
    requests_per_page: int = 3
) -> Dict[str, float]:
    """
    Benchmarks para comparar performance com e sem pool
    
    Returns:
        DicionÃ¡rio com mÃ©tricas de performance
    """
    print("ðŸ§ª BENCHMARK: Pool vs Sem Pool")
    print("=" * 50)
    
    results = {}
    
    # Teste sem pool (mÃ©todo tradicional)
    print("ðŸ“Š Testando SEM pool de conexÃµes...")
    start_time = time.time()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        for page_num in range(pages_to_test):
            for req_num in range(requests_per_page):
                page = await browser.new_page()
                await page.goto("https://www.catho.com.br/vagas/home-office/")
                await page.wait_for_timeout(1000)
                await page.close()
        
        await browser.close()
    
    no_pool_time = time.time() - start_time
    results['without_pool'] = no_pool_time
    print(f"   Tempo total: {no_pool_time:.2f}s")
    
    # Teste com pool
    print("ðŸ“Š Testando COM pool de conexÃµes...")
    start_time = time.time()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        pool = ConnectionPool(min_size=2, max_size=5)
        await pool.initialize(browser)
        
        for page_num in range(pages_to_test):
            for req_num in range(requests_per_page):
                async with PooledPageManager() as page:
                    await page.goto("https://www.catho.com.br/vagas/home-office/")
                    await page.wait_for_timeout(1000)
        
        await pool.shutdown()
        await browser.close()
    
    pool_time = time.time() - start_time
    results['with_pool'] = pool_time
    print(f"   Tempo total: {pool_time:.2f}s")
    
    # Calcular melhoria
    improvement = ((no_pool_time - pool_time) / no_pool_time) * 100
    results['improvement_percent'] = improvement
    results['time_saved'] = no_pool_time - pool_time
    
    print(f"\nðŸš€ RESULTADOS:")
    print(f"   Melhoria: {improvement:.1f}%")
    print(f"   Tempo economizado: {results['time_saved']:.2f}s")
    print(f"   Por requisiÃ§Ã£o: {results['time_saved'] / (pages_to_test * requests_per_page):.3f}s")
    
    return results