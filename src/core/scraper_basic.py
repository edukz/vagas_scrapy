"""
Scraper B√°sico - Vers√£o sem depend√™ncias ML

Scraper simplificado que funciona sem scikit-learn, numpy avan√ßado
ou outras depend√™ncias de machine learning.
"""

import asyncio
import time
from typing import List, Dict
from playwright.async_api import async_playwright
from ..utils.menu_system import Colors


async def scrape_catho_jobs_basic(
    max_concurrent_jobs: int = 3, 
    max_pages: int = 5,
    multi_mode: bool = False
) -> List[Dict]:
    """
    Scraping b√°sico sem depend√™ncias ML
    
    Args:
        multi_mode: Se True, busca em home office + presencial + h√≠brido
                   Se False, apenas home office
    """
    
    # URLs para diferentes modalidades
    if multi_mode:
        urls_to_search = [
            ("Home Office", "https://www.catho.com.br/vagas/home-office/"),
            ("Presencial", "https://www.catho.com.br/vagas/presencial/"),
            ("H√≠brido", "https://www.catho.com.br/vagas/hibrido/"),
        ]
        print(f"{Colors.CYAN}üåç Modo m√∫ltiplas modalidades ativado{Colors.RESET}")
    else:
        urls_to_search = [
            ("Home Office", "https://www.catho.com.br/vagas/home-office/")
        ]
        print(f"{Colors.CYAN}üè† Modo home office exclusivo{Colors.RESET}")
    
    all_jobs = []
    
    try:
        async with async_playwright() as p:
            print(f"{Colors.GREEN}üöÄ Iniciando navegador (modo b√°sico)...{Colors.RESET}")
            
            # Configura√ß√£o b√°sica do browser
            browser = await p.chromium.launch(
                headless=True,  # Modo headless para performance
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu'
                ]
            )
            
            # Criar p√°gina principal
            page = await browser.new_page()
            page.set_default_timeout(20000)
            
            try:
                # Processar cada modalidade
                for mode_name, base_url in urls_to_search:
                    print(f"\n{Colors.YELLOW}üéØ === MODALIDADE: {mode_name.upper()} ==={Colors.RESET}")
                    
                    mode_jobs = await scrape_basic_mode(
                        page, base_url, mode_name, max_pages
                    )
                    
                    if mode_jobs:
                        all_jobs.extend(mode_jobs)
                        print(f"{Colors.GREEN}‚úÖ {mode_name}: {len(mode_jobs)} vagas coletadas{Colors.RESET}")
                    else:
                        print(f"{Colors.YELLOW}‚ö†Ô∏è {mode_name}: Nenhuma vaga encontrada{Colors.RESET}")
                
                # Fechar p√°gina
                await page.close()
                
                print(f"\n{Colors.GREEN}üéâ BUSCA B√ÅSICA CONCLU√çDA!{Colors.RESET}")
                print(f"{Colors.CYAN}{'‚ïê' * 60}{Colors.RESET}")
                print(f"üìä Total coletado: {len(all_jobs)} vagas")
                
                return all_jobs
                
            except Exception as scraping_error:
                print(f"{Colors.RED}‚ùå Erro durante scraping: {scraping_error}{Colors.RESET}")
                return all_jobs
            
            finally:
                try:
                    await browser.close()
                    print(f"{Colors.GRAY}üîÑ Browser fechado{Colors.RESET}")
                except:
                    pass
    
    except Exception as e:
        if "Executable doesn't exist" in str(e):
            print(f"{Colors.RED}‚ùå ERRO: Navegadores do Playwright n√£o encontrados!{Colors.RESET}")
            print(f"{Colors.YELLOW}üìã SOLU√á√ÉO:{Colors.RESET}")
            print("   1. Abra o prompt do Windows (cmd)")
            print("   2. Execute: python -m playwright install") 
            print("   3. Aguarde a instala√ß√£o dos navegadores")
            print("   4. Execute o script novamente")
        else:
            print(f"{Colors.RED}‚ùå Erro inesperado: {e}{Colors.RESET}")
        
        return []


async def scrape_basic_mode(
    page, 
    base_url: str, 
    mode_name: str, 
    max_pages: int
) -> List[Dict]:
    """Scraping b√°sico de uma modalidade espec√≠fica"""
    
    mode_jobs = []
    
    for current_page in range(1, min(max_pages + 1, 4)):  # Limitado a 3 p√°ginas para modo b√°sico
        print(f"\n{Colors.CYAN}üìÑ {mode_name} - P√°gina {current_page}{Colors.RESET}")
        
        try:
            # Construir URL da p√°gina
            if current_page == 1:
                page_url = base_url
            else:
                page_url = f"{base_url}?page={current_page}"
            
            print(f"üåê Navegando para: {page_url}")
            
            # Navegar com timeout mais baixo
            try:
                await page.goto(page_url, wait_until='domcontentloaded', timeout=20000)
                await page.wait_for_timeout(2000)
            except Exception as nav_error:
                print(f"‚ö†Ô∏è Erro na navega√ß√£o: {nav_error}")
                continue
            
            # Extrair vagas da p√°gina
            page_jobs = await extract_basic_jobs(page, mode_name)
            
            if page_jobs:
                mode_jobs.extend(page_jobs)
                print(f"{Colors.GREEN}‚úÖ {mode_name} P{current_page}: {len(page_jobs)} vagas{Colors.RESET}")
            else:
                print(f"{Colors.YELLOW}‚ö†Ô∏è {mode_name} P{current_page}: Nenhuma vaga{Colors.RESET}")
                break
            
            # Delay entre p√°ginas
            await asyncio.sleep(1)
        
        except Exception as page_error:
            print(f"{Colors.RED}‚ùå Erro na p√°gina {current_page} de {mode_name}: {page_error}{Colors.RESET}")
            continue
    
    return mode_jobs


async def extract_basic_jobs(page, mode_name: str) -> List[Dict]:
    """Extrai vagas b√°sicas da p√°gina"""
    
    jobs = []
    
    try:
        print(f"{Colors.GRAY}üîç Procurando vagas em {mode_name}...{Colors.RESET}")
        
        # Aguardar elementos carregarem com timeout baixo
        try:
            await page.wait_for_selector('a[href*="/vagas/"]', timeout=5000)
        except:
            print(f"{Colors.YELLOW}‚ö†Ô∏è Elementos n√£o encontrados rapidamente{Colors.RESET}")
        
        # Seletores b√°sicos para encontrar vagas
        selectors_to_try = [
            'a[href*="/vagas/"]',
            'h2 a',
            '.job-title a',
            '[data-testid*="job"] a'
        ]
        
        job_elements = []
        for selector in selectors_to_try:
            try:
                elements = await page.query_selector_all(selector)
                if elements and len(elements) > 2:  # Pelo menos algumas vagas
                    job_elements = elements
                    break
            except:
                continue
        
        if not job_elements:
            return []
        
        print(f"{Colors.GRAY}üìù Processando {len(job_elements)} elementos de {mode_name}...{Colors.RESET}")
        
        # Processar elementos (m√°ximo 15 por p√°gina no modo b√°sico)
        processed = 0
        for element in job_elements[:15]:
            try:
                # Extrair link
                link = await element.get_attribute('href')
                if not link or 'vagas' not in link:
                    continue
                
                # Garantir URL absoluta
                if link.startswith('/'):
                    link = f"https://www.catho.com.br{link}"
                
                # Extrair t√≠tulo
                title_text = await element.text_content()
                if not title_text or len(title_text.strip()) < 3:
                    title_text = 'Vaga n√£o identificada'
                
                # Limpar t√≠tulo
                title_text = title_text.strip()[:100]
                
                # Criar objeto de vaga b√°sico
                job = {
                    'titulo': title_text,
                    'link': link,
                    'empresa': 'Empresa n√£o identificada',
                    'localizacao': mode_name,
                    'salario': 'N√£o informado',
                    'regime': mode_name,
                    'modalidade_trabalho': mode_name,
                    'nivel': 'N√£o especificado',
                    'tecnologias_detectadas': [],
                    'data_coleta': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'fonte': f'catho_basic_{mode_name.lower().replace(" ", "_")}',
                    'fonte_categoria': mode_name,
                    'tipo_coleta': 'b√°sica_sem_ml'
                }
                
                jobs.append(job)
                processed += 1
                
                # Limite para modo b√°sico
                if processed >= 10:
                    break
                
            except Exception as element_error:
                continue
        
        print(f"{Colors.GREEN}‚úÖ {mode_name}: {len(jobs)} vagas extra√≠das (modo b√°sico){Colors.RESET}")
        return jobs
        
    except Exception as e:
        print(f"{Colors.RED}‚ùå Erro na extra√ß√£o b√°sica de {mode_name}: {e}{Colors.RESET}")
        return []


async def check_basic_catho_accessibility() -> bool:
    """
    Verifica√ß√£o b√°sica de acessibilidade do Catho
    """
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            page.set_default_timeout(10000)
            
            try:
                await page.goto("https://www.catho.com.br/", wait_until='domcontentloaded')
                title = await page.title()
                is_accessible = "catho" in title.lower()
                await browser.close()
                return is_accessible
            except:
                await browser.close()
                return False
    except:
        return False