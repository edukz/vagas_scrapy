import asyncio
from playwright.async_api import async_playwright
import json
from datetime import datetime, timedelta
import re
import hashlib
import os
import time
from typing import List, Dict, Optional

class CacheEntry:
    """
    Entrada do cache com metadados
    """
    def __init__(self, data: Dict, timestamp: datetime, url: str, hash_key: str):
        self.data = data
        self.timestamp = timestamp
        self.url = url
        self.hash_key = hash_key
    
    def is_expired(self, max_age_hours: int = 6) -> bool:
        """Verifica se o cache expirou"""
        return datetime.now() - self.timestamp > timedelta(hours=max_age_hours)
    
    def to_dict(self) -> Dict:
        """Converte para dicion√°rio serializ√°vel"""
        return {
            'data': self.data,
            'timestamp': self.timestamp.isoformat(),
            'url': self.url,
            'hash_key': self.hash_key
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'CacheEntry':
        """Cria inst√¢ncia a partir de dicion√°rio"""
        return cls(
            data=data['data'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            url=data['url'],
            hash_key=data['hash_key']
        )

class IntelligentCache:
    """
    Sistema de cache inteligente para URLs e dados de vagas
    """
    def __init__(self, cache_dir: str = "cache", max_age_hours: int = 6):
        self.cache_dir = cache_dir
        self.max_age_hours = max_age_hours
        self.memory_cache = {}
        
        # Criar diret√≥rio de cache
        os.makedirs(cache_dir, exist_ok=True)
        
        # Limpar cache expirado na inicializa√ß√£o (s√≠ncronamente)
        self._cleanup_expired_cache_sync()
    
    def _get_cache_key(self, url: str) -> str:
        """Gera chave √∫nica para URL"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _get_cache_file_path(self, cache_key: str) -> str:
        """Gera caminho do arquivo de cache"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    async def get(self, url: str) -> Optional[Dict]:
        """
        Recupera dados do cache se v√°lidos
        """
        cache_key = self._get_cache_key(url)
        
        # Verificar cache em mem√≥ria primeiro
        if cache_key in self.memory_cache:
            entry = self.memory_cache[cache_key]
            if not entry.is_expired(self.max_age_hours):
                print(f"‚úì Cache hit (mem√≥ria): {url[:50]}...")
                return entry.data
            else:
                del self.memory_cache[cache_key]
        
        # Verificar cache em disco
        cache_file = self._get_cache_file_path(cache_key)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                entry = CacheEntry.from_dict(cache_data)
                
                if not entry.is_expired(self.max_age_hours):
                    # Carregar de volta para mem√≥ria
                    self.memory_cache[cache_key] = entry
                    print(f"‚úì Cache hit (disco): {url[:50]}...")
                    return entry.data
                else:
                    # Cache expirado, remover arquivo
                    os.remove(cache_file)
            except Exception as e:
                print(f"‚ö† Erro ao ler cache: {e}")
        
        return None
    
    async def set(self, url: str, data: Dict) -> None:
        """
        Armazena dados no cache
        """
        cache_key = self._get_cache_key(url)
        entry = CacheEntry(
            data=data,
            timestamp=datetime.now(),
            url=url,
            hash_key=cache_key
        )
        
        # Armazenar em mem√≥ria
        self.memory_cache[cache_key] = entry
        
        # Armazenar em disco
        cache_file = self._get_cache_file_path(cache_key)
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(entry.to_dict(), f, ensure_ascii=False, indent=2)
            print(f"‚úì Cache salvo: {url[:50]}...")
        except Exception as e:
            print(f"‚ö† Erro ao salvar cache: {e}")
    
    async def _cleanup_expired_cache(self) -> None:
        """
        Remove entradas de cache expiradas
        """
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    cache_file = os.path.join(self.cache_dir, filename)
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        entry = CacheEntry.from_dict(cache_data)
                        if entry.is_expired(self.max_age_hours):
                            os.remove(cache_file)
                            print(f"üóëÔ∏è Cache expirado removido: {filename}")
                    except:
                        # Se houver erro ao ler, remover arquivo corrompido
                        os.remove(cache_file)
        except Exception as e:
            print(f"‚ö† Erro na limpeza do cache: {e}")
    
    def _cleanup_expired_cache_sync(self) -> None:
        """
        Remove entradas de cache expiradas (vers√£o s√≠ncrona)
        """
        try:
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    cache_file = os.path.join(self.cache_dir, filename)
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        entry = CacheEntry.from_dict(cache_data)
                        if entry.is_expired(self.max_age_hours):
                            os.remove(cache_file)
                    except:
                        # Se houver erro ao ler, remover arquivo corrompido
                        os.remove(cache_file)
        except Exception as e:
            pass  # Silencioso na inicializa√ß√£o

class RateLimiter:
    """
    Sistema de rate limiting autom√°tico e adaptativo
    """
    def __init__(self, 
                 requests_per_second: float = 2.0,
                 burst_limit: int = 5,
                 adaptive: bool = True):
        self.requests_per_second = requests_per_second
        self.burst_limit = burst_limit
        self.adaptive = adaptive
        
        self.request_times = []
        self.consecutive_errors = 0
        self.base_delay = 1.0 / requests_per_second
        self.current_delay = self.base_delay
        
    async def acquire(self) -> None:
        """
        Aguarda permiss√£o para fazer uma requisi√ß√£o
        """
        now = time.time()
        
        # Remove requisi√ß√µes antigas (√∫ltimos 60 segundos)
        self.request_times = [t for t in self.request_times if now - t < 60]
        
        # Verifica se excedeu o burst limit
        recent_requests = len([t for t in self.request_times if now - t < 1.0])
        
        if recent_requests >= self.burst_limit:
            wait_time = self.current_delay * (recent_requests - self.burst_limit + 1)
            print(f"‚è≥ Rate limiting: aguardando {wait_time:.2f}s...")
            await asyncio.sleep(wait_time)
        
        # Aplica delay adaptativo
        if self.adaptive and self.current_delay > self.base_delay:
            await asyncio.sleep(self.current_delay - self.base_delay)
        
        self.request_times.append(time.time())
    
    def report_success(self) -> None:
        """
        Reporta sucesso - reduz delay adaptativo
        """
        if self.adaptive and self.consecutive_errors > 0:
            self.consecutive_errors = max(0, self.consecutive_errors - 1)
            self.current_delay = max(
                self.base_delay,
                self.current_delay * 0.9  # Reduz delay gradualmente
            )
    
    def report_error(self) -> None:
        """
        Reporta erro - aumenta delay adaptativo
        """
        if self.adaptive:
            self.consecutive_errors += 1
            self.current_delay = min(
                10.0,  # M√°ximo de 10 segundos
                self.current_delay * (1.5 + self.consecutive_errors * 0.1)
            )
            print(f"‚ö† Erro detectado. Ajustando delay para {self.current_delay:.2f}s")

class PerformanceMonitor:
    """
    Monitor de performance para otimiza√ß√£o autom√°tica
    """
    def __init__(self):
        self.start_time = None
        self.processed_jobs = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.cache_hits = 0
        self.cache_misses = 0
    
    def start_monitoring(self):
        """Inicia monitoramento"""
        self.start_time = time.time()
        print("üìä Monitoramento de performance iniciado")
    
    def record_job_processed(self):
        """Registra vaga processada"""
        self.processed_jobs += 1
    
    def record_request_success(self):
        """Registra requisi√ß√£o bem-sucedida"""
        self.successful_requests += 1
    
    def record_request_failure(self):
        """Registra falha de requisi√ß√£o"""
        self.failed_requests += 1
    
    def record_cache_hit(self):
        """Registra acerto de cache"""
        self.cache_hits += 1
    
    def record_cache_miss(self):
        """Registra perda de cache"""
        self.cache_misses += 1
    
    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas de performance"""
        if not self.start_time:
            return {}
        
        elapsed_time = time.time() - self.start_time
        total_requests = self.successful_requests + self.failed_requests
        
        return {
            'tempo_execucao': f"{elapsed_time:.2f}s",
            'vagas_processadas': self.processed_jobs,
            'vagas_por_segundo': f"{self.processed_jobs / elapsed_time:.2f}" if elapsed_time > 0 else "0",
            'requisicoes_totais': total_requests,
            'taxa_sucesso': f"{(self.successful_requests / total_requests * 100):.1f}%" if total_requests > 0 else "0%",
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'taxa_cache': f"{(self.cache_hits / (self.cache_hits + self.cache_misses) * 100):.1f}%" if (self.cache_hits + self.cache_misses) > 0 else "0%"
        }
    
    def print_stats(self):
        """Imprime estat√≠sticas formatadas"""
        stats = self.get_stats()
        if stats:
            print(f"\nüìä ESTAT√çSTICAS DE PERFORMANCE:")
            print(f"   ‚è±Ô∏è  Tempo de execu√ß√£o: {stats['tempo_execucao']}")
            print(f"   üìã Vagas processadas: {stats['vagas_processadas']}")
            print(f"   ‚ö° Vagas/segundo: {stats['vagas_por_segundo']}")
            print(f"   üåê Requisi√ß√µes totais: {stats['requisicoes_totais']}")
            print(f"   ‚úÖ Taxa de sucesso: {stats['taxa_sucesso']}")
            print(f"   üíæ Cache hits: {stats['cache_hits']}")
            print(f"   ‚ùå Cache misses: {stats['cache_misses']}")
            print(f"   üìà Efici√™ncia do cache: {stats['taxa_cache']}")

class JobFilter:
    """
    Sistema de filtragem avan√ßada para vagas
    """
    def __init__(self):
        # Tecnologias comuns
        self.tecnologias_comuns = [
            'python', 'javascript', 'java', 'react', 'angular', 'vue', 'node',
            'django', 'flask', 'fastapi', 'sql', 'mysql', 'postgresql', 'mongodb',
            'aws', 'azure', 'docker', 'kubernetes', 'git', 'html', 'css',
            'typescript', 'php', 'c#', 'c++', 'golang', 'rust', 'scala',
            'spring', 'hibernate', 'redis', 'elasticsearch', 'jenkins', 'ci/cd'
        ]
        
        # N√≠veis de experi√™ncia
        self.niveis_experiencia = [
            'j√∫nior', 'junior', 'pleno', 's√™nior', 'senior', 'especialista',
            'trainee', 'estagi√°rio', 'coordenador', 'l√≠der', 'gerente'
        ]
        
        # Tipos de empresa
        self.tipos_empresa = [
            'startup', 'multinacional', 'consultoria', 'banco', 'fintech',
            'e-commerce', 'sa√∫de', 'educa√ß√£o', 'governo', 'ong', 'ag√™ncia'
        ]

    def extract_technologies(self, job):
        """
        Extrai tecnologias mencionadas na vaga
        """
        technologies = set()
        
        # Textos para analisar
        texts_to_analyze = [
            job.get('titulo', '').lower(),
            job.get('descricao', '').lower(),
            job.get('requisitos', '').lower()
        ]
        
        full_text = ' '.join(texts_to_analyze)
        
        for tech in self.tecnologias_comuns:
            if tech.lower() in full_text:
                technologies.add(tech.lower())
        
        return list(technologies)

    def extract_salary_range(self, job):
        """
        Extrai e padroniza informa√ß√µes de sal√°rio
        """
        salary_text = job.get('salario', '').lower()
        
        if not salary_text or salary_text in ['a combinar', 'n√£o informado']:
            return None
        
        # Procurar por valores num√©ricos
        import re
        numbers = re.findall(r'(\d+\.?\d*)', salary_text.replace('.', '').replace(',', '.'))
        
        if numbers:
            try:
                # Assumir que valores acima de 1000 s√£o sal√°rios mensais
                values = [float(n) for n in numbers if float(n) > 1000]
                if values:
                    return {
                        'min': min(values),
                        'max': max(values) if len(values) > 1 else min(values),
                        'original': job.get('salario', '')
                    }
            except:
                pass
        
        return {'original': job.get('salario', '')}

    def categorize_experience_level(self, job):
        """
        Categoriza o n√≠vel de experi√™ncia
        """
        texts_to_analyze = [
            job.get('titulo', '').lower(),
            job.get('nivel_experiencia', '').lower(),
            job.get('requisitos', '').lower()
        ]
        
        full_text = ' '.join(texts_to_analyze)
        
        if any(nivel in full_text for nivel in ['trainee', 'estagi√°rio', 'est√°gio']):
            return 'trainee'
        elif any(nivel in full_text for nivel in ['j√∫nior', 'junior']):
            return 'junior'
        elif any(nivel in full_text for nivel in ['pleno']):
            return 'pleno'
        elif any(nivel in full_text for nivel in ['s√™nior', 'senior', 'sr']):
            return 'senior'
        elif any(nivel in full_text for nivel in ['especialista', 'expert']):
            return 'especialista'
        elif any(nivel in full_text for nivel in ['coordenador', 'l√≠der', 'lead']):
            return 'lideranca'
        elif any(nivel in full_text for nivel in ['gerente', 'diretor']):
            return 'gestao'
        else:
            return 'nao_especificado'

    def categorize_company_type(self, job):
        """
        Categoriza o tipo de empresa
        """
        empresa = job.get('empresa', '').lower()
        descricao = job.get('descricao', '').lower()
        
        full_text = f"{empresa} {descricao}"
        
        if any(tipo in full_text for tipo in ['startup', 'scale-up']):
            return 'startup'
        elif any(tipo in full_text for tipo in ['banco', 'financeira', 'fintech']):
            return 'financeiro'
        elif any(tipo in full_text for tipo in ['consultoria', 'consulting']):
            return 'consultoria'
        elif any(tipo in full_text for tipo in ['sa√∫de', 'hospital', 'medicina']):
            return 'saude'
        elif any(tipo in full_text for tipo in ['educa√ß√£o', 'universidade', 'ensino']):
            return 'educacao'
        elif any(tipo in full_text for tipo in ['e-commerce', 'marketplace', 'varejo']):
            return 'ecommerce'
        elif any(tipo in full_text for tipo in ['governo', 'p√∫blico', 'municipal']):
            return 'governo'
        else:
            return 'nao_categorizado'

    def apply_filters(self, jobs, filters_config):
        """
        Aplica filtros √†s vagas baseado na configura√ß√£o
        """
        filtered_jobs = []
        
        for job in jobs:
            # Enriquecer job com dados de an√°lise
            job['tecnologias_detectadas'] = self.extract_technologies(job)
            job['faixa_salarial'] = self.extract_salary_range(job)
            job['nivel_categorizado'] = self.categorize_experience_level(job)
            job['tipo_empresa'] = self.categorize_company_type(job)
            
            # Aplicar filtros
            if self._job_matches_filters(job, filters_config):
                filtered_jobs.append(job)
        
        return filtered_jobs

    def _job_matches_filters(self, job, filters_config):
        """
        Verifica se uma vaga atende aos filtros
        """
        # Filtro por tecnologias
        if filters_config.get('tecnologias'):
            required_techs = [tech.lower() for tech in filters_config['tecnologias']]
            job_techs = job.get('tecnologias_detectadas', [])
            if not any(tech in job_techs for tech in required_techs):
                return False
        
        # Filtro por sal√°rio m√≠nimo
        if filters_config.get('salario_minimo'):
            job_salary = job.get('faixa_salarial')
            if not job_salary or not job_salary.get('min'):
                return False
            if job_salary['min'] < filters_config['salario_minimo']:
                return False
        
        # Filtro por n√≠vel de experi√™ncia
        if filters_config.get('niveis_experiencia'):
            if job.get('nivel_categorizado') not in filters_config['niveis_experiencia']:
                return False
        
        # Filtro por tipo de empresa
        if filters_config.get('tipos_empresa'):
            if job.get('tipo_empresa') not in filters_config['tipos_empresa']:
                return False
        
        # Filtro por palavras-chave no t√≠tulo/descri√ß√£o
        if filters_config.get('palavras_chave'):
            full_text = f"{job.get('titulo', '')} {job.get('descricao', '')}".lower()
            keywords = [kw.lower() for kw in filters_config['palavras_chave']]
            if not any(kw in full_text for kw in keywords):
                return False
        
        return True

def get_filter_configuration():
    """
    Interface para configurar filtros
    """
    print("\n=== CONFIGURA√á√ÉO DE FILTROS AVAN√áADOS ===\n")
    
    filters = {}
    
    # Filtro por tecnologias
    print("1. FILTRO POR TECNOLOGIAS")
    print("Tecnologias dispon√≠veis: python, javascript, react, angular, node, java, etc.")
    tech_input = input("Digite as tecnologias desejadas (separadas por v√≠rgula, ou ENTER para pular): ").strip()
    if tech_input:
        filters['tecnologias'] = [tech.strip() for tech in tech_input.split(',')]
        print(f"‚úì Filtro aplicado: {filters['tecnologias']}")
    
    # Filtro por sal√°rio
    print("\n2. FILTRO POR SAL√ÅRIO M√çNIMO")
    salary_input = input("Digite o sal√°rio m√≠nimo desejado (ou ENTER para pular): ").strip()
    if salary_input:
        try:
            filters['salario_minimo'] = float(salary_input)
            print(f"‚úì Sal√°rio m√≠nimo: R$ {filters['salario_minimo']}")
        except:
            print("‚ö† Valor inv√°lido, filtro ignorado")
    
    # Filtro por n√≠vel
    print("\n3. FILTRO POR N√çVEL DE EXPERI√äNCIA")
    print("Op√ß√µes: trainee, junior, pleno, senior, especialista, lideranca, gestao")
    level_input = input("Digite os n√≠veis desejados (separados por v√≠rgula, ou ENTER para pular): ").strip()
    if level_input:
        filters['niveis_experiencia'] = [level.strip() for level in level_input.split(',')]
        print(f"‚úì N√≠veis: {filters['niveis_experiencia']}")
    
    # Filtro por tipo de empresa
    print("\n4. FILTRO POR TIPO DE EMPRESA")
    print("Op√ß√µes: startup, financeiro, consultoria, saude, educacao, ecommerce, governo")
    company_input = input("Digite os tipos desejados (separados por v√≠rgula, ou ENTER para pular): ").strip()
    if company_input:
        filters['tipos_empresa'] = [tipo.strip() for tipo in company_input.split(',')]
        print(f"‚úì Tipos de empresa: {filters['tipos_empresa']}")
    
    # Filtro por palavras-chave
    print("\n5. FILTRO POR PALAVRAS-CHAVE")
    keyword_input = input("Digite palavras-chave para buscar no t√≠tulo/descri√ß√£o (separadas por v√≠rgula, ou ENTER para pular): ").strip()
    if keyword_input:
        filters['palavras_chave'] = [kw.strip() for kw in keyword_input.split(',')]
        print(f"‚úì Palavras-chave: {filters['palavras_chave']}")
    
    print(f"\n‚úì Configura√ß√£o de filtros conclu√≠da!")
    if filters:
        print("Filtros ativos:")
        for key, value in filters.items():
            print(f"  - {key}: {value}")
    else:
        print("Nenhum filtro ser√° aplicado (todas as vagas ser√£o retornadas)")
    
    return filters

class PageNavigator:
    """
    Sistema de navega√ß√£o inteligente por m√∫ltiplas p√°ginas
    """
    def __init__(self, max_pages: int = 5):
        self.max_pages = max_pages
        self.current_page = 1
        self.total_pages_found = 0
        
    async def detect_pagination_type(self, page) -> str:
        """
        Detecta o tipo de pagina√ß√£o do site
        """
        # Verificar pagina√ß√£o tradicional
        pagination_selectors = [
            'nav[aria-label*="paginat"], nav[aria-label*="Paginat"]',
            '.pagination',
            '[class*="pagination"]',
            '[class*="pager"]',
            'nav:has(a[href*="page"])',
            'div:has(a[href*="page"])'
        ]
        
        for selector in pagination_selectors:
            pagination_elem = await page.query_selector(selector)
            if pagination_elem:
                return "traditional"
        
        # Verificar scroll infinito
        scroll_indicators = [
            '[class*="load-more"]',
            '[class*="infinite"]',
            '[class*="scroll"]',
            'button:has-text("Ver mais")',
            'button:has-text("Carregar mais")'
        ]
        
        for selector in scroll_indicators:
            scroll_elem = await page.query_selector(selector)
            if scroll_elem:
                return "infinite_scroll"
        
        # Verificar bot√£o "pr√≥xima p√°gina"
        next_selectors = [
            'a:has-text("Pr√≥xima")',
            'a:has-text(">")',
            'a[aria-label*="pr√≥xima"]',
            'a[aria-label*="next"]',
            'button:has-text("Pr√≥xima")',
            '[class*="next"]'
        ]
        
        for selector in next_selectors:
            next_elem = await page.query_selector(selector)
            if next_elem:
                return "next_button"
        
        return "single_page"
    
    async def get_page_numbers(self, page) -> List[int]:
        """
        Extrai n√∫meros de p√°ginas dispon√≠veis
        """
        page_numbers = []
        
        print("üîç Debug: Procurando n√∫meros de p√°ginas...")
        
        # Tentar encontrar links numerados com mais seletores
        number_selectors = [
            'a[href*="page="]',
            'a[href*="p="]',
            '.pagination a',
            '[class*="pagination"] a',
            '[class*="pager"] a',
            'nav a',
            'a:has-text("2")',
            'a:has-text("3")',
            'a:has-text("4")',
            'a:has-text("5")',
            'button[class*="page"]',
            '[data-page]'
        ]
        
        for i, selector in enumerate(number_selectors):
            try:
                elements = await page.query_selector_all(selector)
                print(f"   Seletor {i+1} '{selector[:30]}...': {len(elements)} elementos encontrados")
                
                for element in elements:
                    try:
                        text = await element.inner_text()
                        print(f"   - Texto: '{text.strip()}'")
                        
                        if text.strip().isdigit():
                            num = int(text.strip())
                            page_numbers.append(num)
                            print(f"   ‚úì N√∫mero de p√°gina encontrado: {num}")
                        
                        # Tamb√©m verificar no href
                        href = await element.get_attribute('href')
                        if href:
                            print(f"   - Href: '{href}'")
                            import re
                            matches = re.findall(r'(?:page=|p=)(\d+)', href)
                            for match in matches:
                                num = int(match)
                                page_numbers.append(num)
                                print(f"   ‚úì N√∫mero no href: {num}")
                    except Exception as e:
                        continue
            except Exception as e:
                print(f"   ‚ùå Erro no seletor: {e}")
                continue
        
        # Se n√£o encontrou p√°ginas numeradas, tentar uma abordagem mais simples
        if not page_numbers:
            print("üîç Debug: Tentando encontrar pr√≥xima p√°gina...")
            next_selectors = [
                'a:has-text("Pr√≥xima")',
                'a:has-text(">")',
                'button:has-text(">")',
                'a[rel="next"]',
                '[class*="next"]'
            ]
            
            for selector in next_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    if elements:
                        print(f"   ‚úì Encontrado bot√£o pr√≥xima p√°gina: {len(elements)} elementos")
                        # Se h√° bot√£o pr√≥xima, assumir pelo menos 2 p√°ginas e tentar at√© max_pages
                        return list(range(1, min(self.max_pages + 1, 6)))  # Assumir at√© 5 p√°ginas
                except:
                    continue
        
        unique_pages = sorted(list(set(page_numbers)))
        print(f"üéØ Debug: N√∫meros de p√°ginas encontrados: {unique_pages}")
        return unique_pages
    
    async def navigate_to_page(self, page, page_number: int, base_url: str) -> bool:
        """
        Navega para uma p√°gina espec√≠fica
        """
        try:
            print(f"üîÑ Debug: Tentando navegar para p√°gina {page_number}")
            
            # Tentar diferentes formatos de URL de pagina√ß√£o (mais op√ß√µes)
            url_patterns = [
                f"{base_url}?page={page_number}",
                f"{base_url}?p={page_number}",
                f"{base_url}&page={page_number}",
                f"{base_url}&p={page_number}",
                f"{base_url}/page/{page_number}/",
                f"{base_url}/p{page_number}/",
                f"{base_url}#{page_number}",
                f"{base_url}?pageIndex={page_number}",
                f"{base_url}?pageNumber={page_number}",
                f"{base_url}?offset={page_number-1}",
                f"{base_url}?start={page_number-1}"
            ]
            
            for i, url in enumerate(url_patterns):
                try:
                    print(f"   üåê Tentativa {i+1}: {url}")
                    
                    # Navegar para a URL
                    response = await page.goto(url, wait_until='domcontentloaded', timeout=20000)
                    print(f"   üìä Status HTTP: {response.status if response else 'N/A'}")
                    
                    # Aguardar carregamento
                    await page.wait_for_timeout(3000)
                    
                    # Verificar se a p√°gina carregou com conte√∫do
                    jobs_found = await page.query_selector_all('h2 a[href*="/vagas/"]')
                    print(f"   üìã Vagas encontradas: {len(jobs_found)}")
                    
                    # Verificar se mudou de p√°gina (URL atual)
                    current_url = page.url
                    print(f"   üîó URL atual: {current_url}")
                    
                    if len(jobs_found) > 0:
                        # Verificar se o conte√∫do realmente mudou
                        page_title = await page.title()
                        print(f"   üìÑ T√≠tulo: {page_title}")
                        
                        # Verificar se h√° indicador da p√°gina atual
                        page_indicators = await page.query_selector_all(f'[class*="active"]:has-text("{page_number}"), [class*="current"]:has-text("{page_number}"), .selected:has-text("{page_number}")')
                        print(f"   üéØ Indicadores de p√°gina atual: {len(page_indicators)}")
                        
                        print(f"   ‚úÖ P√°gina {page_number} carregada com sucesso!")
                        return True
                    else:
                        print(f"   ‚ö† Nenhuma vaga encontrada nesta URL")
                        
                except Exception as e:
                    print(f"   ‚ùå Erro na tentativa {i+1}: {e}")
                    continue
            
            # Se chegou aqui, tentar clique no link da p√°gina
            print(f"üîÑ Debug: Tentando clicar no link da p√°gina {page_number}")
            try:
                # Voltar para p√°gina base
                await page.goto(base_url, wait_until='domcontentloaded', timeout=20000)
                await page.wait_for_timeout(2000)
                
                # Procurar link clic√°vel para a p√°gina
                page_link_selectors = [
                    f'a:has-text("{page_number}")',
                    f'button:has-text("{page_number}")',
                    f'[data-page="{page_number}"]',
                    f'a[href*="page={page_number}"]',
                    f'a[href*="p={page_number}"]'
                ]
                
                for selector in page_link_selectors:
                    try:
                        page_link = await page.query_selector(selector)
                        if page_link:
                            print(f"   üîó Encontrado link clic√°vel: {selector}")
                            await page_link.click()
                            await page.wait_for_load_state('domcontentloaded')
                            await page.wait_for_timeout(3000)
                            
                            # Verificar se funcionou
                            jobs_found = await page.query_selector_all('h2 a[href*="/vagas/"]')
                            if len(jobs_found) > 0:
                                print(f"   ‚úÖ Clique funcionou! {len(jobs_found)} vagas encontradas")
                                return True
                    except Exception as e:
                        continue
                        
            except Exception as e:
                print(f"   ‚ùå Erro ao tentar clique: {e}")
            
            print(f"   ‚ùå Falha ao navegar para p√°gina {page_number}")
            return False
            
        except Exception as e:
            print(f"‚ùå Erro geral ao navegar para p√°gina {page_number}: {e}")
            return False
    
    async def try_next_page_button(self, page) -> bool:
        """
        Tenta clicar no bot√£o "pr√≥xima p√°gina"
        """
        next_selectors = [
            'a:has-text("Pr√≥xima")',
            'a:has-text(">")',
            'a[aria-label*="pr√≥xima"]',
            'a[aria-label*="next"]',
            'button:has-text("Pr√≥xima")',
            '[class*="next"]:not([class*="disabled"])',
            'a[rel="next"]'
        ]
        
        for selector in next_selectors:
            try:
                next_elem = await page.query_selector(selector)
                if next_elem:
                    # Verificar se o elemento est√° vis√≠vel e clic√°vel
                    is_visible = await next_elem.is_visible()
                    is_disabled = await next_elem.get_attribute('disabled')
                    
                    if is_visible and not is_disabled:
                        print(f"üîÑ Clicando no bot√£o pr√≥xima p√°gina...")
                        await next_elem.click()
                        await page.wait_for_load_state('networkidle')
                        await page.wait_for_timeout(2000)
                        return True
            except:
                continue
        
        return False
    
    async def try_infinite_scroll(self, page) -> bool:
        """
        Tenta carregar mais conte√∫do via scroll infinito
        """
        try:
            # Fazer scroll at√© o final da p√°gina
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await page.wait_for_timeout(3000)
            
            # Procurar e clicar em bot√µes "carregar mais"
            load_more_selectors = [
                'button:has-text("Ver mais")',
                'button:has-text("Carregar mais")',
                'button:has-text("Load more")',
                '[class*="load-more"]',
                '[class*="show-more"]'
            ]
            
            for selector in load_more_selectors:
                try:
                    load_button = await page.query_selector(selector)
                    if load_button and await load_button.is_visible():
                        print(f"üîÑ Carregando mais vagas...")
                        await load_button.click()
                        await page.wait_for_timeout(3000)
                        return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            print(f"‚ùå Erro no scroll infinito: {e}")
            return False

async def extract_jobs_from_current_page(page, seen_urls: set) -> List[Dict]:
    """
    Extrai vagas da p√°gina atual
    """
    jobs = []
    
    try:
        job_elements = await page.query_selector_all('h2 a[href*="/vagas/"]')
        
        for element in job_elements:
            try:
                job_title = await element.inner_text()
                job_link = await element.get_attribute('href')
                
                if not job_link or job_link in seen_urls:
                    continue
                
                if not re.search(r'/vagas/[^/]+/\d+/$', job_link):
                    continue
                
                skip_patterns = [
                    '/vagas/por-area/',
                    '/vagas/por-local/',
                    '/empresas/',
                    'buscar-vagas'
                ]
                
                if any(pattern in job_link for pattern in skip_patterns):
                    continue
                
                # Garantir URL completa
                if job_link.startswith('/'):
                    job_link = f"https://www.catho.com.br{job_link}"
                
                job_data = {
                    'titulo': job_title.strip(),
                    'link': job_link
                }
                
                jobs.append(job_data)
                seen_urls.add(job_link)
                
            except Exception as e:
                continue
        
    except Exception as e:
        print(f"‚ùå Erro ao extrair vagas da p√°gina: {e}")
    
    return jobs

async def extract_basic_info_from_jobs(page, jobs: List[Dict]) -> List[Dict]:
    """
    Extrai informa√ß√µes b√°sicas das vagas da p√°gina de listagem
    """
    print(f"\nüìã Extraindo informa√ß√µes b√°sicas de {len(jobs)} vagas...")
    
    for i, job in enumerate(jobs):
        try:
            # Tentar encontrar o article que cont√©m a vaga
            article_selectors = [
                f'article:has(a[href*="{job["link"].split("/")[-2]}"])',
                f'div:has(a[href="{job["link"]}"])',
                f'[class*="job"]:has(a[href="{job["link"]}"])'
            ]
            
            article = None
            for selector in article_selectors:
                try:
                    article = await page.query_selector(selector)
                    if article:
                        break
                except:
                    continue
            
            if article:
                # Empresa
                empresa_selectors = [
                    'span.sc-gEvEer', 
                    '[class*="company"]', 
                    '[class*="empresa"]',
                    'span:has-text("Ltda")',
                    'span:has-text("S.A.")',
                    'div[class*="company"]'
                ]
                
                for selector in empresa_selectors:
                    try:
                        empresa_elem = await article.query_selector(selector)
                        if empresa_elem:
                            empresa = await empresa_elem.inner_text()
                            if empresa and len(empresa.strip()) > 2:
                                job['empresa'] = empresa.strip()
                                break
                    except:
                        continue
                
                if 'empresa' not in job:
                    job['empresa'] = 'N√£o informada'
                
                # Localiza√ß√£o b√°sica
                local_selectors = [
                    'button[title*="Local"]', 
                    '[class*="location"]', 
                    '[class*="local"]',
                    'span:has-text("Home Office")',
                    'span:has-text("Remoto")',
                    '[class*="cidade"]'
                ]
                
                for selector in local_selectors:
                    try:
                        local_elem = await article.query_selector(selector)
                        if local_elem:
                            local = await local_elem.inner_text()
                            if local and len(local.strip()) > 2:
                                job['localizacao'] = local.strip()
                                break
                    except:
                        continue
                
                if 'localizacao' not in job:
                    job['localizacao'] = 'Home Office'
            else:
                job['empresa'] = 'N√£o informada'
                job['localizacao'] = 'Home Office'
                
        except Exception as e:
            job['empresa'] = 'N√£o informada'
            job['localizacao'] = 'Home Office'
            
        # Progress indicator
        if (i + 1) % 20 == 0:
            print(f"  üìù {i + 1}/{len(jobs)} vagas processadas...")
    
    print(f"‚úÖ Informa√ß√µes b√°sicas extra√≠das de {len(jobs)} vagas")
    return jobs

async def scrape_job_details(page, job_url, cache: IntelligentCache = None, 
                            rate_limiter: RateLimiter = None, 
                            monitor: PerformanceMonitor = None):
    """
    Extrai informa√ß√µes detalhadas de uma vaga espec√≠fica com cache e rate limiting
    """
    try:
        # Verificar cache primeiro
        if cache:
            cached_data = await cache.get(job_url)
            if cached_data:
                if monitor:
                    monitor.record_cache_hit()
                return cached_data
            elif monitor:
                monitor.record_cache_miss()
        
        # Aplicar rate limiting
        if rate_limiter:
            await rate_limiter.acquire()
        
        # Fazer requisi√ß√£o
        await page.goto(job_url, wait_until='networkidle', timeout=30000)
        await page.wait_for_timeout(2000)
        
        job_details = {}
        
        # Descri√ß√£o completa
        try:
            desc_selectors = [
                '[data-testid="job-description"]',
                '.job-description',
                '[class*="description"]',
                '.sc-gEvEer',
                'section:has-text("Descri√ß√£o")',
                'div:has-text("Descri√ß√£o")'
            ]
            
            for selector in desc_selectors:
                desc_elem = await page.query_selector(selector)
                if desc_elem:
                    job_details['descricao'] = (await desc_elem.inner_text()).strip()
                    break
        except:
            job_details['descricao'] = 'N√£o encontrada'
        
        # Sal√°rio/faixa salarial
        try:
            salary_selectors = [
                '[data-testid="salary"]',
                '.salary',
                '[class*="salario"]',
                '[class*="remuneracao"]',
                'span:has-text("R$")',
                'div:has-text("Sal√°rio")'
            ]
            
            for selector in salary_selectors:
                salary_elem = await page.query_selector(selector)
                if salary_elem:
                    salary_text = await salary_elem.inner_text()
                    if 'R$' in salary_text or 'sal√°rio' in salary_text.lower():
                        job_details['salario'] = salary_text.strip()
                        break
        except:
            pass
        
        if 'salario' not in job_details:
            job_details['salario'] = 'A combinar'
        
        # Requisitos t√©cnicos
        try:
            req_selectors = [
                'section:has-text("Requisitos")',
                'div:has-text("Requisitos")',
                '[class*="requirements"]',
                '[class*="requisitos"]',
                'section:has-text("Qualifica√ß√µes")',
                'div:has-text("Qualifica√ß√µes")'
            ]
            
            for selector in req_selectors:
                req_elem = await page.query_selector(selector)
                if req_elem:
                    job_details['requisitos'] = (await req_elem.inner_text()).strip()
                    break
        except:
            job_details['requisitos'] = 'N√£o especificados'
        
        # Benef√≠cios
        try:
            ben_selectors = [
                'section:has-text("Benef√≠cios")',
                'div:has-text("Benef√≠cios")',
                '[class*="benefits"]',
                '[class*="beneficios"]',
                'section:has-text("Oferecemos")',
                'ul li'
            ]
            
            for selector in ben_selectors:
                ben_elem = await page.query_selector(selector)
                if ben_elem:
                    ben_text = await ben_elem.inner_text()
                    if any(word in ben_text.lower() for word in ['benef√≠cio', 'vale', 'plano', 'conv√™nio', 'aux√≠lio']):
                        job_details['beneficios'] = ben_text.strip()
                        break
        except:
            job_details['beneficios'] = 'N√£o informados'
        
        # N√≠vel de experi√™ncia
        try:
            exp_selectors = [
                '[class*="experience"]',
                '[class*="nivel"]',
                'span:has-text("anos")',
                'div:has-text("Experi√™ncia")',
                'section:has-text("Experi√™ncia")'
            ]
            
            for selector in exp_selectors:
                exp_elem = await page.query_selector(selector)
                if exp_elem:
                    exp_text = await exp_elem.inner_text()
                    if any(word in exp_text.lower() for word in ['j√∫nior', 'pleno', 's√™nior', 'anos', 'experi√™ncia']):
                        job_details['nivel_experiencia'] = exp_text.strip()
                        break
        except:
            job_details['nivel_experiencia'] = 'N√£o especificado'
        
        # Modalidade de trabalho
        try:
            mode_selectors = [
                '[class*="work-mode"]',
                '[class*="modalidade"]',
                'span:has-text("Home")',
                'span:has-text("Remoto")',
                'span:has-text("Presencial")',
                'div:has-text("Modalidade")'
            ]
            
            for selector in mode_selectors:
                mode_elem = await page.query_selector(selector)
                if mode_elem:
                    mode_text = await mode_elem.inner_text()
                    if any(word in mode_text.lower() for word in ['home', 'remoto', 'presencial', 'h√≠brido']):
                        job_details['modalidade'] = mode_text.strip()
                        break
        except:
            job_details['modalidade'] = 'Home Office'
        
        # Data de publica√ß√£o
        try:
            date_selectors = [
                '[class*="date"]',
                '[class*="publicada"]',
                'time',
                'span:has-text("dia")',
                'span:has-text("publicada")',
                'div:has-text("Publicada")'
            ]
            
            for selector in date_selectors:
                date_elem = await page.query_selector(selector)
                if date_elem:
                    date_text = await date_elem.inner_text()
                    if any(word in date_text.lower() for word in ['dia', 'publicada', 'h√°', 'ontem', 'hoje']):
                        job_details['data_publicacao'] = date_text.strip()
                        break
        except:
            job_details['data_publicacao'] = 'N√£o informada'
        
        # Salvar no cache se bem-sucedido
        if cache:
            await cache.set(job_url, job_details)
        
        # Reportar sucesso
        if rate_limiter:
            rate_limiter.report_success()
        if monitor:
            monitor.record_request_success()
        
        return job_details
        
    except Exception as e:
        print(f"Erro ao extrair detalhes da vaga: {e}")
        
        # Reportar erro
        if rate_limiter:
            rate_limiter.report_error()
        if monitor:
            monitor.record_request_failure()
        
        return {
            'descricao': 'Erro ao carregar',
            'salario': 'Erro ao carregar',
            'requisitos': 'Erro ao carregar',
            'beneficios': 'Erro ao carregar',
            'nivel_experiencia': 'Erro ao carregar',
            'modalidade': 'Erro ao carregar',
            'data_publicacao': 'Erro ao carregar'
        }

async def scrape_catho_jobs(max_concurrent_jobs: int = 3, max_pages: int = 5):
    """
    Faz o scraping das vagas home office no site da Catho com navega√ß√£o por m√∫ltiplas p√°ginas
    """
    base_url = "https://www.catho.com.br/vagas/home-office/"
    
    # Inicializar sistemas de performance
    cache = IntelligentCache(max_age_hours=6)
    rate_limiter = RateLimiter(requests_per_second=1.5, burst_limit=3, adaptive=True)
    monitor = PerformanceMonitor()
    navigator = PageNavigator(max_pages=max_pages)
    monitor.start_monitoring()
    
    async with async_playwright() as p:
        # Iniciando o navegador
        print("üöÄ Iniciando navegador otimizado com navega√ß√£o multi-p√°gina...")
        browser = await p.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-sandbox'
            ]
        )
        
        # Criando p√°ginas
        page = await browser.new_page()
        
        # Pool de p√°ginas para processamento paralelo
        detail_pages = []
        for i in range(max_concurrent_jobs):
            detail_page = await browser.new_page()
            detail_pages.append(detail_page)
        
        try:
            print(f"üåê Iniciando coleta de m√∫ltiplas p√°ginas (m√°x: {max_pages} p√°ginas)")
            
            all_jobs = []
            seen_urls = set()
            current_page = 1
            
            # Primeira p√°gina
            print(f"\nüìÑ === P√ÅGINA {current_page} ===")
            await page.goto(base_url, wait_until='networkidle', timeout=60000)
            await page.wait_for_timeout(3000)
            
            title = await page.title()
            print(f"T√≠tulo da p√°gina: {title}")
            
            # Detectar tipo de pagina√ß√£o
            pagination_type = await navigator.detect_pagination_type(page)
            print(f"üîç Tipo de pagina√ß√£o detectado: {pagination_type}")
            
            # Processar primeira p√°gina
            page_jobs = await extract_jobs_from_current_page(page, seen_urls)
            all_jobs.extend(page_jobs)
            print(f"‚úÖ P√°gina {current_page}: {len(page_jobs)} vagas coletadas")
            
            # Navegar pelas p√°ginas restantes
            if pagination_type != "single_page" and max_pages > 1:
                if pagination_type == "traditional":
                    # Detectar n√∫mero total de p√°ginas
                    page_numbers = await navigator.get_page_numbers(page)
                    
                    if page_numbers:
                        max_available = min(max(page_numbers), max_pages)
                        print(f"üìä P√°ginas dispon√≠veis detectadas: {page_numbers[:10]}... (processando at√© {max_available})")
                        pages_to_try = list(range(2, max_available + 1))
                    else:
                        print("‚ö† N√∫meros de p√°ginas n√£o detectados, tentando navega√ß√£o for√ßada...")
                        pages_to_try = list(range(2, max_pages + 1))
                    
                    for page_num in pages_to_try:
                        print(f"\nüìÑ === P√ÅGINA {page_num} ===")
                        success = await navigator.navigate_to_page(page, page_num, base_url)
                        
                        if success:
                            page_jobs = await extract_jobs_from_current_page(page, seen_urls)
                            all_jobs.extend(page_jobs)
                            print(f"‚úÖ P√°gina {page_num}: {len(page_jobs)} vagas coletadas")
                            
                            if len(page_jobs) == 0:
                                print("‚ö† P√°gina sem vagas, parando navega√ß√£o")
                                break
                        else:
                            print(f"‚ùå Falha ao carregar p√°gina {page_num}")
                            # N√£o quebrar imediatamente, tentar pr√≥xima p√°gina
                            continue
                        
                        # Pequena pausa entre p√°ginas
                        await asyncio.sleep(1)
                
                elif pagination_type == "next_button":
                    # Usar bot√£o "pr√≥xima p√°gina"
                    for page_num in range(2, max_pages + 1):
                        print(f"\nüìÑ === P√ÅGINA {page_num} ===")
                        success = await navigator.try_next_page_button(page)
                        
                        if success:
                            page_jobs = await extract_jobs_from_current_page(page, seen_urls)
                            all_jobs.extend(page_jobs)
                            print(f"‚úÖ P√°gina {page_num}: {len(page_jobs)} vagas coletadas")
                            
                            if len(page_jobs) == 0:
                                print("‚ö† P√°gina sem vagas, parando navega√ß√£o")
                                break
                        else:
                            print(f"‚ùå N√£o foi poss√≠vel navegar para p√°gina {page_num}")
                            break
                        
                        await asyncio.sleep(1)
                
                elif pagination_type == "infinite_scroll":
                    # Usar scroll infinito
                    attempts = 0
                    while attempts < max_pages - 1:
                        print(f"\nüîÑ === CARREGAMENTO {attempts + 2} ===")
                        initial_count = len(all_jobs)
                        
                        success = await navigator.try_infinite_scroll(page)
                        if success:
                            page_jobs = await extract_jobs_from_current_page(page, seen_urls)
                            new_jobs = [job for job in page_jobs if job not in all_jobs]
                            all_jobs.extend(new_jobs)
                            
                            print(f"‚úÖ Carregamento {attempts + 2}: {len(new_jobs)} novas vagas")
                            
                            if len(new_jobs) == 0:
                                print("‚ö† Nenhuma vaga nova carregada, parando")
                                break
                        else:
                            print("‚ùå N√£o foi poss√≠vel carregar mais conte√∫do")
                            break
                        
                        attempts += 1
                        await asyncio.sleep(2)
            
            print(f"\nüéØ COLETA CONCLU√çDA: {len(all_jobs)} vagas de {current_page if pagination_type == 'single_page' else max_pages} p√°ginas")
            
            # Extrair informa√ß√µes b√°sicas de todas as vagas
            jobs = await extract_basic_info_from_jobs(page, all_jobs)
            
            # Terceira passada: extrair informa√ß√µes detalhadas em paralelo
            print(f"\nüîÑ Processamento paralelo de {len(jobs)} vagas (m√°x {max_concurrent_jobs} simult√¢neas)...")
            
            semaphore = asyncio.Semaphore(max_concurrent_jobs)
            
            async def process_job_with_semaphore(job, page_index):
                async with semaphore:
                    page_to_use = detail_pages[page_index % len(detail_pages)]
                    
                    try:
                        # Extrair informa√ß√µes detalhadas
                        details = await scrape_job_details(
                            page_to_use, job['link'], 
                            cache, rate_limiter, monitor
                        )
                        
                        # Adicionar informa√ß√µes detalhadas ao job
                        job.update(details)
                        monitor.record_job_processed()
                        
                        print(f"  ‚úÖ [{job.get('titulo', 'Sem t√≠tulo')[:30]}...] Processado com sucesso")
                        return job
                        
                    except Exception as e:
                        print(f"  ‚ùå Erro ao processar vaga: {e}")
                        monitor.record_request_failure()
                        
                        # Adicionar valores padr√£o em caso de erro
                        job.update({
                            'descricao': 'Erro ao carregar',
                            'salario': 'A combinar',
                            'requisitos': 'N√£o especificados',
                            'beneficios': 'N√£o informados',
                            'nivel_experiencia': 'N√£o especificado',
                            'modalidade': 'Home Office',
                            'data_publicacao': 'N√£o informada'
                        })
                        return job
            
            # Processar jobs em lotes paralelos
            tasks = []
            for i, job in enumerate(jobs):
                task = process_job_with_semaphore(job, i)
                tasks.append(task)
            
            # Executar todas as tasks em paralelo
            print(f"‚ö° Iniciando processamento paralelo...")
            start_parallel = time.time()
            
            processed_jobs = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_parallel = time.time()
            parallel_time = end_parallel - start_parallel
            
            print(f"‚ú® Processamento paralelo conclu√≠do em {parallel_time:.2f}s")
            print(f"‚ö° Velocidade: {len(jobs)/parallel_time:.2f} vagas/segundo")
            
            # Filtrar jobs v√°lidos (n√£o exce√ß√µes)
            valid_jobs = [job for job in processed_jobs if not isinstance(job, Exception)]
            
            # Mostrar estat√≠sticas de performance
            monitor.print_stats()
            
            return valid_jobs
            
        except Exception as e:
            print(f"Erro durante o scraping: {e}")
            
            # Salvando HTML para debug
            try:
                html = await page.content()
                with open("catho_error.html", "w", encoding="utf-8") as f:
                    f.write(html)
                print("HTML da p√°gina salvo em 'catho_error.html' para an√°lise")
            except:
                pass
                
            return []
        
        finally:
            # Fechar todas as p√°ginas
            for detail_page in detail_pages:
                await detail_page.close()
            
            print("\nüìä Processamento conclu√≠do!")
            print("Pressione Enter para fechar o navegador...")
            input()
            await browser.close()

class FileManager:
    """
    Gerenciador inteligente de arquivos - evita spam e organiza resultados
    """
    def __init__(self, results_dir: str = "resultados"):
        self.results_dir = results_dir
        self.max_files_per_type = 5  # Manter apenas os 5 mais recentes de cada tipo
        
        # Criar diret√≥rio se n√£o existir
        os.makedirs(results_dir, exist_ok=True)
        
        # Subdiret√≥rios organizados
        self.subdirs = {
            'json': os.path.join(results_dir, 'json'),
            'txt': os.path.join(results_dir, 'txt'), 
            'csv': os.path.join(results_dir, 'csv'),
            'relatorios': os.path.join(results_dir, 'relatorios')
        }
        
        for subdir in self.subdirs.values():
            os.makedirs(subdir, exist_ok=True)
    
    def cleanup_old_files(self, file_pattern: str, max_files: int = 5):
        """Remove arquivos antigos mantendo apenas os mais recentes"""
        try:
            import glob
            files = glob.glob(file_pattern)
            if len(files) > max_files:
                # Ordenar por data de modifica√ß√£o (mais antigos primeiro)
                files.sort(key=os.path.getmtime)
                files_to_remove = files[:-max_files]
                
                for file_path in files_to_remove:
                    os.remove(file_path)
                    print(f"üóëÔ∏è Arquivo antigo removido: {os.path.basename(file_path)}")
        except Exception as e:
            pass  # Silencioso para n√£o poluir logs
    
    def get_latest_filename(self, base_name: str, extension: str, subdir: str) -> str:
        """Gera nome de arquivo inteligente - sobrescreve se for do mesmo dia"""
        today = datetime.now().strftime("%Y%m%d")
        
        # Verificar se j√° existe arquivo de hoje
        existing_pattern = os.path.join(self.subdirs[subdir], f"{base_name}_{today}*{extension}")
        
        try:
            import glob
            existing_files = glob.glob(existing_pattern)
            if existing_files:
                # Usar o arquivo existente de hoje (sobrescrever)
                return existing_files[0]
        except:
            pass
        
        # Criar novo arquivo com timestamp completo apenas se necess√°rio
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        return os.path.join(self.subdirs[subdir], f"{base_name}_{timestamp}{extension}")

def save_results(jobs, filters_applied=None, ask_user_preference=True):
    """
    Salva os resultados de forma organizada e inteligente
    """
    if not jobs:
        print("Nenhuma vaga para salvar")
        return
    
    # Perguntar prefer√™ncia do usu√°rio
    if ask_user_preference:
        print("\nüíæ OP√á√ïES DE SALVAMENTO:")
        print("1. Arquivo √∫nico (sobrescreve arquivo do dia)")
        print("2. Arquivo com timestamp (mant√©m hist√≥rico)")
        print("3. N√£o salvar arquivos (apenas exibir no terminal)")
        
        choice = input("Escolha uma op√ß√£o (1-3, padr√£o: 1): ").strip() or "1"
        
        if choice == "3":
            print("‚úì Resultados n√£o salvos em arquivo")
            return
        
        use_timestamp = choice == "2"
    else:
        use_timestamp = False
    
    file_manager = FileManager()
    
    if use_timestamp:
        # Modo com timestamp (hist√≥rico completo)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = os.path.join(file_manager.subdirs['json'], f"vagas_catho_{timestamp}.json")
        txt_filename = os.path.join(file_manager.subdirs['txt'], f"vagas_catho_{timestamp}.txt")
        csv_filename = os.path.join(file_manager.subdirs['csv'], f"vagas_catho_{timestamp}.csv")
        stats_filename = os.path.join(file_manager.subdirs['relatorios'], f"analise_{timestamp}.txt")
    else:
        # Modo arquivo √∫nico (sobrescreve arquivo do dia)
        json_filename = file_manager.get_latest_filename("vagas_catho", ".json", "json")
        txt_filename = file_manager.get_latest_filename("vagas_catho", ".txt", "txt")
        csv_filename = file_manager.get_latest_filename("vagas_catho", ".csv", "csv") 
        stats_filename = file_manager.get_latest_filename("analise_completa", ".txt", "relatorios")
    
    # Salvar JSON
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)
    print(f"\n‚úì Salvo: {os.path.relpath(json_filename)}")
    
    # Salvar TXT formatado de forma limpa e organizada
    with open(txt_filename, 'w', encoding='utf-8') as f:
        # Cabe√ßalho limpo
        f.write("VAGAS DE EMPREGO HOME OFFICE - CATHO\n")
        f.write("=" * 50 + "\n")
        f.write(f"Data da coleta: {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}\n")
        f.write(f"Total de vagas: {len(jobs)}\n")
        
        if filters_applied:
            f.write(f"\nFiltros aplicados: {', '.join(f'{k}: {v}' for k, v in filters_applied.items())}\n")
        
        f.write("\n" + "=" * 50 + "\n\n")
        
        # Lista das vagas de forma mais limpa
        for i, job in enumerate(jobs, 1):
            # Cabe√ßalho da vaga mais simples
            f.write(f"{i:2d}. {job['titulo']}\n")
            f.write("-" * (len(f"{i:2d}. {job['titulo']}")) + "\n")
            
            # Informa√ß√µes organizadas em tabela simples
            empresa = job.get('empresa', 'N√£o informada')
            localizacao = job.get('localizacao', 'N√£o informada')
            salario = job.get('salario', 'N√£o informado')
            nivel = job.get('nivel_categorizado', 'N√£o especificado').replace('_', ' ').title()
            
            f.write(f"Empresa:     {empresa}\n")
            f.write(f"Local:       {localizacao}\n")
            f.write(f"Sal√°rio:     {salario}\n")
            f.write(f"N√≠vel:       {nivel}\n")
            
            # Tecnologias em uma linha
            if job.get('tecnologias_detectadas'):
                techs = ', '.join(job['tecnologias_detectadas'])
                f.write(f"Tecnologias: {techs}\n")
            
            # Link
            f.write(f"Link:        {job['link']}\n")
            
            # Separador entre vagas
            f.write("\n" + "." * 50 + "\n\n")
        
        # Resumo estat√≠stico no final
        f.write("RESUMO ESTAT√çSTICO\n")
        f.write("=" * 50 + "\n\n")
        
        # Tecnologias mais usadas
        all_techs = {}
        for job in jobs:
            for tech in job.get('tecnologias_detectadas', []):
                all_techs[tech] = all_techs.get(tech, 0) + 1
        
        if all_techs:
            f.write("Tecnologias mais demandadas:\n")
            for tech, count in sorted(all_techs.items(), key=lambda x: x[1], reverse=True)[:10]:
                f.write(f"  {tech}: {count} vagas\n")
            f.write("\n")
        
        # Empresas com mais vagas
        empresas = {}
        for job in jobs:
            empresa = job.get('empresa', 'N√£o informada')
            if empresa != 'N√£o informada':
                empresas[empresa] = empresas.get(empresa, 0) + 1
        
        if empresas:
            f.write("Empresas com mais vagas:\n")
            for empresa, count in sorted(empresas.items(), key=lambda x: x[1], reverse=True)[:10]:
                f.write(f"  {empresa}: {count} vagas\n")
            f.write("\n")
        
        # Distribui√ß√£o por n√≠vel
        niveis = {}
        for job in jobs:
            nivel = job.get('nivel_categorizado', 'N√£o especificado').replace('_', ' ').title()
            niveis[nivel] = niveis.get(nivel, 0) + 1
        
        f.write("Distribui√ß√£o por n√≠vel:\n")
        for nivel, count in sorted(niveis.items(), key=lambda x: x[1], reverse=True):
            f.write(f"  {nivel}: {count} vagas\n")
    
    print(f"‚úì Salvo: {os.path.relpath(txt_filename)}")
    
    # Salvar CSV limpo e organizado
    try:
        import csv
        
        # Preparar dados limpos para CSV
        csv_data = []
        for job in jobs:
            # Limpar e organizar dados
            clean_row = {
                'Titulo': job.get('titulo', '').strip(),
                'Empresa': job.get('empresa', 'N√£o informada').strip(),
                'Localizacao': job.get('localizacao', 'N√£o informada').strip(),
                'Salario': job.get('salario', 'N√£o informado').strip(),
                'Salario_Min': job.get('faixa_salarial', {}).get('min', '') if job.get('faixa_salarial') else '',
                'Salario_Max': job.get('faixa_salarial', {}).get('max', '') if job.get('faixa_salarial') else '',
                'Nivel_Experiencia': job.get('nivel_categorizado', 'N√£o especificado').replace('_', ' ').title(),
                'Tipo_Empresa': job.get('tipo_empresa', 'N√£o categorizado').replace('_', ' ').title(),
                'Modalidade': job.get('modalidade', 'N√£o especificada').strip(),
                'Data_Publicacao': job.get('data_publicacao', 'N√£o informada').strip(),
                'Tecnologias': ', '.join(job.get('tecnologias_detectadas', [])),
                'Tem_Beneficios': 'Sim' if job.get('beneficios') and job.get('beneficios') != 'N√£o informados' else 'N√£o',
                'Link': job.get('link', '').strip()
            }
            
            # Adicionar apenas campos √∫teis para an√°lise
            csv_data.append(clean_row)
        
        # Salvar CSV com cabe√ßalhos em portugu√™s
        with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as f:  # BOM para Excel
            if csv_data:
                fieldnames = [
                    'Titulo', 'Empresa', 'Localizacao', 'Salario', 
                    'Salario_Min', 'Salario_Max', 'Nivel_Experiencia', 
                    'Tipo_Empresa', 'Modalidade', 'Data_Publicacao', 
                    'Tecnologias', 'Tem_Beneficios', 'Link'
                ]
                
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(csv_data)
        
        print(f"‚úì Salvo: {os.path.relpath(csv_filename)} (otimizado para Excel)")
    except Exception as e:
        print(f"‚ö† Erro ao salvar CSV: {e}")
    
    # Salvar relat√≥rio estat√≠stico simplificado
    with open(stats_filename, 'w', encoding='utf-8') as f:
        f.write("RELAT√ìRIO DE AN√ÅLISE - VAGAS HOME OFFICE\n")
        f.write("=" * 45 + "\n")
        f.write(f"Data: {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}\n")
        f.write(f"Total de vagas: {len(jobs)}\n\n")
        
        if filters_applied:
            f.write("Filtros aplicados:\n")
            for key, value in filters_applied.items():
                f.write(f"  ‚Ä¢ {key}: {', '.join(value) if isinstance(value, list) else value}\n")
            f.write("\n")
        
        # An√°lise de tecnologias
        all_techs = {}
        for job in jobs:
            for tech in job.get('tecnologias_detectadas', []):
                all_techs[tech] = all_techs.get(tech, 0) + 1
        
        if all_techs:
            f.write("TECNOLOGIAS MAIS DEMANDADAS\n")
            f.write("-" * 30 + "\n")
            for tech, count in sorted(all_techs.items(), key=lambda x: x[1], reverse=True)[:10]:
                percentage = (count / len(jobs)) * 100
                f.write(f"{tech:<15} {count:>3} vagas ({percentage:>4.1f}%)\n")
            f.write("\n")
        
        # An√°lise salarial
        salaries = []
        for job in jobs:
            if job.get('faixa_salarial') and job['faixa_salarial'].get('min'):
                salaries.append(job['faixa_salarial']['min'])
        
        if salaries:
            f.write("AN√ÅLISE SALARIAL\n")
            f.write("-" * 15 + "\n")
            f.write(f"Menor sal√°rio:    R$ {min(salaries):>8,.2f}\n")
            f.write(f"Maior sal√°rio:    R$ {max(salaries):>8,.2f}\n")
            f.write(f"Sal√°rio m√©dio:    R$ {sum(salaries)/len(salaries):>8,.2f}\n")
            f.write(f"Com sal√°rio info: {len(salaries):>3} de {len(jobs)} vagas ({(len(salaries)/len(jobs)*100):>4.1f}%)\n\n")
        
        # Top empresas
        empresas = {}
        for job in jobs:
            empresa = job.get('empresa', 'N√£o informada')
            if empresa != 'N√£o informada':
                empresas[empresa] = empresas.get(empresa, 0) + 1
        
        if empresas:
            f.write("EMPRESAS COM MAIS VAGAS\n")
            f.write("-" * 23 + "\n")
            for empresa, count in sorted(empresas.items(), key=lambda x: x[1], reverse=True)[:8]:
                f.write(f"{empresa:<25} {count:>2} vagas\n")
            f.write("\n")
        
        # Distribui√ß√µes
        nivel_counts = {}
        tipo_counts = {}
        
        for job in jobs:
            nivel = job.get('nivel_categorizado', 'N√£o especificado').replace('_', ' ').title()
            nivel_counts[nivel] = nivel_counts.get(nivel, 0) + 1
            
            tipo = job.get('tipo_empresa', 'N√£o categorizado').replace('_', ' ').title()
            tipo_counts[tipo] = tipo_counts.get(tipo, 0) + 1
        
        f.write("DISTRIBUI√á√ÉO POR N√çVEL\n")
        f.write("-" * 21 + "\n")
        for nivel, count in sorted(nivel_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(jobs)) * 100
            f.write(f"{nivel:<20} {count:>3} vagas ({percentage:>4.1f}%)\n")
        f.write("\n")
        
        f.write("TIPOS DE EMPRESA\n")
        f.write("-" * 16 + "\n")
        for tipo, count in sorted(tipo_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(jobs)) * 100
            f.write(f"{tipo:<20} {count:>3} vagas ({percentage:>4.1f}%)\n")
    
    print(f"‚úì Salvo: {os.path.relpath(stats_filename)}")
    
    # Limpeza autom√°tica de arquivos antigos (se usando timestamp)
    if use_timestamp:
        file_manager.cleanup_old_files(
            os.path.join(file_manager.subdirs['json'], "vagas_catho_*.json"), 5
        )
        file_manager.cleanup_old_files(
            os.path.join(file_manager.subdirs['txt'], "vagas_catho_*.txt"), 5
        )
        file_manager.cleanup_old_files(
            os.path.join(file_manager.subdirs['csv'], "vagas_catho_*.csv"), 5
        )
        file_manager.cleanup_old_files(
            os.path.join(file_manager.subdirs['relatorios'], "analise_*.txt"), 5
        )
    
    print(f"\nüìÅ Arquivos organizados no diret√≥rio: {file_manager.results_dir}/")
    print(f"   üìÑ {len(jobs)} vagas salvas em 3 formatos + relat√≥rio de an√°lise")

async def main():
    """
    Fun√ß√£o principal
    """
    print("=== WEB SCRAPER CATHO (VERS√ÉO COMPLETA OTIMIZADA) ===\n")
    print("Esta vers√£o inclui:")
    print("üîç Sistema de filtragem avan√ßada:")
    print("   ‚Ä¢ Filtros por tecnologias espec√≠ficas")
    print("   ‚Ä¢ Filtro por faixa salarial")
    print("   ‚Ä¢ Filtro por n√≠vel de experi√™ncia")
    print("   ‚Ä¢ Filtro por tipo de empresa")
    print("   ‚Ä¢ Filtro por palavras-chave")
    print("\n‚ö° Otimiza√ß√µes de performance:")
    print("   ‚Ä¢ Processamento paralelo/ass√≠ncrono")
    print("   ‚Ä¢ Cache inteligente (6h de validade)")
    print("   ‚Ä¢ Rate limiting autom√°tico e adaptativo")
    print("   ‚Ä¢ Navega√ß√£o por m√∫ltiplas p√°ginas")
    print("   ‚Ä¢ Monitoramento de performance em tempo real")
    print("\nüìä Coleta de informa√ß√µes completas:")
    print("   ‚Ä¢ Descri√ß√£o completa")
    print("   ‚Ä¢ Sal√°rio/faixa salarial")
    print("   ‚Ä¢ Requisitos t√©cnicos")
    print("   ‚Ä¢ Benef√≠cios")
    print("   ‚Ä¢ N√≠vel de experi√™ncia")
    print("   ‚Ä¢ Modalidade de trabalho")
    print("   ‚Ä¢ Data de publica√ß√£o")
    print("\nüìÅ Gerenciamento inteligente de arquivos:")
    print("   ‚Ä¢ Organiza√ß√£o em subdiret√≥rios")
    print("   ‚Ä¢ Op√ß√£o de arquivo √∫nico ou hist√≥rico")
    print("   ‚Ä¢ Limpeza autom√°tica de arquivos antigos")
    print("   ‚Ä¢ M√∫ltiplos formatos (JSON, TXT, CSV)")
    print("\nüìà An√°lise autom√°tica:")
    print("   ‚Ä¢ Detec√ß√£o de tecnologias")
    print("   ‚Ä¢ Categoriza√ß√£o de empresas")
    print("   ‚Ä¢ An√°lise salarial")
    print("   ‚Ä¢ Relat√≥rios estat√≠sticos\n")
    
    # Configurar filtros
    print("Deseja aplicar filtros √†s vagas? (s/n)")
    apply_filters = input().strip().lower() in ['s', 'sim', 'y', 'yes']
    
    filters_config = {}
    if apply_filters:
        filters_config = get_filter_configuration()
    else:
        print("‚úì Nenhum filtro ser√° aplicado - todas as vagas ser√£o coletadas")
    
    # Configurar performance
    print("\n‚ö° CONFIGURA√á√ÉO DE PERFORMANCE:")
    print("Quantas vagas processar simultaneamente? (1-5, padr√£o: 3)")
    try:
        max_concurrent = int(input("Digite o n√∫mero: ").strip() or "3")
        max_concurrent = max(1, min(5, max_concurrent))
    except:
        max_concurrent = 3
    
    print(f"‚úì Processamento paralelo configurado: {max_concurrent} vagas simult√¢neas")
    
    # Configurar p√°ginas
    print("\nüìÑ CONFIGURA√á√ÉO DE P√ÅGINAS:")
    print("Quantas p√°ginas analisar? (1-10, padr√£o: 5)")
    try:
        max_pages = int(input("Digite o n√∫mero: ").strip() or "5")
        max_pages = max(1, min(10, max_pages))
    except:
        max_pages = 5
    
    print(f"‚úì Navega√ß√£o configurada: at√© {max_pages} p√°ginas")
    print("üìù Nota: Cache inteligente ativo (6h), Rate limiting adaptativo ativo")
    print("üîç Detec√ß√£o autom√°tica de pagina√ß√£o (tradicional/bot√µes/scroll infinito)")
    
    # Executando o scraper
    print(f"\n{'='*60}")
    print("INICIANDO COLETA OTIMIZADA MULTI-P√ÅGINA...")
    print(f"{'='*60}")
    
    jobs = await scrape_catho_jobs(max_concurrent_jobs=max_concurrent, max_pages=max_pages)
    
    if jobs:
        print(f"\n{'='*60}")
        print(f"DADOS COLETADOS: {len(jobs)} vagas encontradas")
        print(f"{'='*60}")
        
        # Aplicar filtros e an√°lise
        if filters_config or True:  # Sempre aplicar an√°lise
            print("\nAplicando filtros e an√°lise...")
            job_filter = JobFilter()
            
            if filters_config:
                filtered_jobs = job_filter.apply_filters(jobs, filters_config)
                print(f"‚úì Filtros aplicados: {len(filtered_jobs)} vagas selecionadas")
            else:
                # Aplicar apenas an√°lise sem filtros
                filtered_jobs = job_filter.apply_filters(jobs, {})
                print(f"‚úì An√°lise aplicada a todas as {len(filtered_jobs)} vagas")
            
            jobs = filtered_jobs
        
        if jobs:
            print(f"\n{'='*60}")
            print(f"RESULTADO FINAL: {len(jobs)} vagas processadas")
            print(f"{'='*60}")
            
            # Salvando resultados
            save_results(jobs, filters_config)
            
            print(f"\n‚úÖ PROCESSAMENTO CONCLU√çDO COM SUCESSO!")
            print(f"üìÅ Arquivos gerados:")
            print(f"   ‚Ä¢ JSON com dados completos e an√°lise")
            print(f"   ‚Ä¢ TXT formatado para leitura")
            print(f"   ‚Ä¢ CSV para an√°lise em planilhas")
            print(f"   ‚Ä¢ Relat√≥rio de an√°lise completa")
            
            # Mostrar preview dos resultados
            print(f"\nüî• PREVIEW DOS RESULTADOS:")
            if jobs:
                # Tecnologias mais demandadas
                all_techs = {}
                for job in jobs:
                    for tech in job.get('tecnologias_detectadas', []):
                        all_techs[tech] = all_techs.get(tech, 0) + 1
                
                if all_techs:
                    print(f"   üíª Top 5 tecnologias:")
                    for tech, count in sorted(all_techs.items(), key=lambda x: x[1], reverse=True)[:5]:
                        print(f"      - {tech}: {count} vagas")
                
                # An√°lise de n√≠veis
                niveis = {}
                for job in jobs:
                    nivel = job.get('nivel_categorizado', 'N√£o categorizado')
                    niveis[nivel] = niveis.get(nivel, 0) + 1
                
                if niveis:
                    print(f"   üìä Distribui√ß√£o por n√≠vel:")
                    for nivel, count in sorted(niveis.items(), key=lambda x: x[1], reverse=True)[:3]:
                        print(f"      - {nivel.title()}: {count} vagas")
        else:
            print("\n‚ö† Nenhuma vaga atendeu aos crit√©rios de filtro especificados.")
            print("Tente ajustar os filtros ou executar sem filtros.")
    else:
        print("\n‚úó Nenhuma vaga foi encontrada no site.")
        print("Verifique o arquivo 'catho_debug.png' para ver o que apareceu na p√°gina")

if __name__ == "__main__":
    asyncio.run(main())