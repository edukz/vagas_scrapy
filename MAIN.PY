import asyncio
import sys
import os

# Adicionar pasta src ao path para imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.scraper import scrape_catho_jobs
from src.filters import JobFilter, get_filter_configuration
from src.utils import save_results
from src.structured_logger import structured_logger, Component


async def main():
    """
    Fun√ß√£o principal - vers√£o modularizada e organizada
    """
    # Inicializar sistema de logs
    structured_logger.log_system_info()
    structured_logger.info(
        "Application started",
        component=Component.MAIN,
        context={'version': '3.0', 'features': ['retry', 'fallback', 'validation', 'logging', 'circuit_breaker', 'metrics', 'alerts']}
    )
    
    print("=== WEB SCRAPER CATHO (VERS√ÉO MODULARIZADA) ===\n")
    print("‚ú® Projeto reorganizado com arquitetura modular:")
    print("   üì¶ cache.py - Sistema de cache inteligente")
    print("   üì¶ scraper.py - L√≥gica de scraping e extra√ß√£o")
    print("   üì¶ filters.py - Sistema de filtros avan√ßados")
    print("   üì¶ navigation.py - Navega√ß√£o multi-p√°gina")
    print("   üõ°Ô∏è retry_system.py - Sistema de retry autom√°tico")
    print("   üéØ selector_fallback.py - Fallback de seletores")
    print("   üìã data_validator.py - Valida√ß√£o robusta de dados")
    print("   üìù structured_logger.py - Logs estruturados")
    print("   üîß circuit_breaker.py - Prote√ß√£o contra sobrecarga")
    print("   üìä metrics_tracker.py - Monitoramento em tempo real")
    print("   üö® alert_system.py - Alertas autom√°ticos")
    print("   üì¶ utils.py - Utilit√°rios e performance")
    print("   üì¶ main.py - Interface principal (este arquivo)")
    
    print("\nüîç Recursos dispon√≠veis:")
    print("   ‚Ä¢ Sistema de filtragem avan√ßada")
    print("   ‚Ä¢ Cache inteligente (6h de validade)")
    print("   ‚Ä¢ Rate limiting adaptativo")
    print("   ‚Ä¢ Navega√ß√£o por m√∫ltiplas p√°ginas")
    print("   ‚Ä¢ Processamento paralelo")
    print("   ‚Ä¢ An√°lise autom√°tica de dados")
    print("   ‚Ä¢ M√∫ltiplos formatos de sa√≠da\n")
    
    # Configurar filtros
    print("Deseja aplicar filtros √†s vagas? (s/n)")
    apply_filters = input().strip().lower() in ['s', 'sim', 'y', 'yes']
    
    filters_config = {}
    if apply_filters:
        filters_config = get_filter_configuration()
    else:
        print("‚úì Nenhum filtro ser√° aplicado - todas as vagas ser√£o coletadas")
    
    # Configurar performance
    print("\n‚ö° CONFIGURA√á√ÉO DE PERFORMANCE:")
    print("Quantas vagas processar simultaneamente? (1-5, padr√£o: 3)")
    try:
        max_concurrent = int(input("Digite o n√∫mero: ").strip() or "3")
        max_concurrent = max(1, min(5, max_concurrent))
    except:
        max_concurrent = 3
    
    print(f"‚úì Processamento paralelo: {max_concurrent} vagas simult√¢neas")
    
    # Configurar p√°ginas
    print("\nüìÑ CONFIGURA√á√ÉO DE P√ÅGINAS:")
    print("Quantas p√°ginas analisar? (1-10, padr√£o: 5)")
    try:
        max_pages = int(input("Digite o n√∫mero: ").strip() or "5")
        max_pages = max(1, min(10, max_pages))
    except:
        max_pages = 5
    
    print(f"‚úì Navega√ß√£o configurada: at√© {max_pages} p√°ginas")
    
    # Op√ß√£o de busca no cache
    print("\nüîç MODO DE OPERA√á√ÉO:")
    print("Escolha uma op√ß√£o:")
    print("  1. Fazer novo scraping")
    print("  2. Buscar no cache existente")
    print("  3. Limpar cache/checkpoint e fazer scraping completo")
    print("  4. Limpar duplicatas em arquivos existentes")
    print("Op√ß√£o (1-4, padr√£o: 1): ", end="")
    
    try:
        mode_choice = input().strip() or "1"
        mode_choice = int(mode_choice)
        mode_choice = max(1, min(4, mode_choice))
    except:
        mode_choice = 1
    
    if mode_choice == 2:
        # Modo de busca no cache
        from src.compressed_cache import CompressedCache
        cache = CompressedCache()
        
        print("\nüîç BUSCA NO CACHE:")
        print("Escolha o tipo de busca:")
        print("  1. Listar todas as entradas")
        print("  2. Buscar por empresa")
        print("  3. Buscar por tecnologia")
        print("  4. Buscar por localiza√ß√£o")
        print("  5. Estat√≠sticas do cache")
        print("  6. Top empresas e tecnologias")
        print("Op√ß√£o (1-6, padr√£o: 1): ", end="")
        
        try:
            search_choice = int(input().strip() or "1")
            search_choice = max(1, min(6, search_choice))
        except:
            search_choice = 1
        
        criteria = {}
        
        if search_choice == 2:  # Buscar por empresa
            company = input("Digite o nome da empresa: ").strip()
            if company:
                criteria['companies'] = [company]
        elif search_choice == 3:  # Buscar por tecnologia
            tech = input("Digite a tecnologia: ").strip()
            if tech:
                criteria['technologies'] = [tech]
        elif search_choice == 4:  # Buscar por localiza√ß√£o
            location = input("Digite a localiza√ß√£o: ").strip()
            if location:
                criteria['locations'] = [location]
        elif search_choice == 5:  # Estat√≠sticas
            cache.print_compression_report()
            cache.index.print_summary()
            return
        elif search_choice == 6:  # Top empresas e tecnologias
            print("\nüè¢ TOP 10 EMPRESAS:")
            top_companies = cache.get_top_companies(10)
            for i, (company, count) in enumerate(top_companies, 1):
                print(f"   {i:2d}. {company}: {count} vagas")
            
            print("\nüíª TOP 10 TECNOLOGIAS:")
            top_techs = cache.get_top_technologies(10)
            for i, (tech, count) in enumerate(top_techs, 1):
                print(f"   {i:2d}. {tech}: {count} vagas")
            return
        
        # Realizar busca
        results = cache.search_cache(criteria)
        
        print(f"\nüìä RESULTADOS DA BUSCA: {len(results)} entradas encontradas")
        print("=" * 60)
        
        for i, entry in enumerate(results[:10], 1):  # Mostrar apenas 10 primeiros
            print(f"\n{i:2d}. {entry.url[:60]}...")
            print(f"     üìÖ Data: {entry.timestamp.strftime('%Y-%m-%d %H:%M')}")
            print(f"     üíº Vagas: {entry.job_count}")
            print(f"     üè¢ Empresas: {', '.join(entry.companies[:3])}{'...' if len(entry.companies) > 3 else ''}")
            print(f"     üíª Tecnologias: {', '.join(entry.technologies[:3])}{'...' if len(entry.technologies) > 3 else ''}")
            print(f"     üìç Localiza√ß√µes: {', '.join(entry.locations[:3])}{'...' if len(entry.locations) > 3 else ''}")
            print(f"     üíæ Tamanho: {entry.file_size / 1024:.1f} KB ‚Üí {entry.compressed_size / 1024:.1f} KB ({entry.compression_ratio:.1f}% compress√£o)")
        
        if len(results) > 10:
            print(f"\n... e mais {len(results) - 10} entradas")
        
        return
    
    elif mode_choice == 3:
        # Modo de limpeza de cache
        print("\nüóëÔ∏è LIMPEZA DE CACHE E CHECKPOINT:")
        print("‚ö†Ô∏è  ATEN√á√ÉO: Isso remover√° todos os dados armazenados!")
        print("Tem certeza? (s/n): ", end="")
        
        confirm = input().strip().lower()
        if confirm in ['s', 'sim', 'y', 'yes']:
            import shutil
            
            # Remover diret√≥rios de cache e checkpoint
            directories_to_clean = [
                "data/cache",
                "data/checkpoints"
            ]
            
            cleaned = 0
            for directory in directories_to_clean:
                if os.path.exists(directory):
                    try:
                        shutil.rmtree(directory)
                        print(f"‚úÖ {directory} removido")
                        cleaned += 1
                    except Exception as e:
                        print(f"‚ùå Erro ao remover {directory}: {e}")
            
            if cleaned > 0:
                print(f"‚úÖ Cache limpo! {cleaned} diret√≥rios removidos")
                print("üîÑ Agora o scraping processar√° todas as p√°ginas do zero")
            else:
                print("‚ÑπÔ∏è  Nenhum cache encontrado para limpar")
        else:
            print("‚ùå Limpeza cancelada")
            return
    
    elif mode_choice == 4:
        # Modo de limpeza de duplicatas
        print("\nüßπ LIMPEZA DE DUPLICATAS:")
        print("Esta opera√ß√£o ir√°:")
        print("  ‚Ä¢ Escanear todos os arquivos JSON em data/")
        print("  ‚Ä¢ Remover vagas duplicadas")
        print("  ‚Ä¢ Criar backup dos arquivos originais (.bak)")
        print("  ‚Ä¢ Exibir relat√≥rio detalhado")
        print("\nDeseja continuar? (s/n): ", end="")
        
        confirm = input().strip().lower()
        if confirm in ['s', 'sim', 'y', 'yes']:
            from src.deduplicator import JobDeduplicator
            
            deduplicator = JobDeduplicator()
            removed_count = deduplicator.clean_existing_files("data")
            
            if removed_count > 0:
                print(f"\n‚úÖ Limpeza conclu√≠da: {removed_count} duplicatas removidas!")
                deduplicator.print_stats()
            else:
                print("‚ÑπÔ∏è  Nenhuma duplicata encontrada ou nenhum arquivo para processar")
        else:
            print("‚ùå Limpeza cancelada")
        
        return
    
    # Configurar otimiza√ß√µes (apenas para novo scraping)
    print("\n‚ö° OTIMIZA√á√ïES DE PERFORMANCE:")
    print("Escolha a vers√£o do scraper:")
    print("  1. B√°sica - Arquitetura modular (sem otimiza√ß√µes)")
    print("  2. Otimizada - Cache comprimido + Incremental (recomendado para dados novos)")
    print("  3. M√°xima Performance - Pool de Conex√µes + Todas otimiza√ß√µes (mais r√°pido)")
    print("Op√ß√£o (1-3, padr√£o: 3): ", end="")
    
    try:
        choice = input().strip() or "3"
        choice = int(choice)
        choice = max(1, min(3, choice))
    except:
        choice = 3
    
    # Executando o scraper
    print(f"\n{'='*60}")
    if choice == 1:
        print("INICIANDO COLETA COM ARQUITETURA MODULAR...")
        print(f"{'='*60}")
        jobs = await scrape_catho_jobs(max_concurrent_jobs=max_concurrent, max_pages=max_pages)
    elif choice == 2:
        print("INICIANDO COLETA OTIMIZADA (COMPRESS√ÉO + INCREMENTAL)...")
        print(f"{'='*60}")
        
        # Op√ß√£o de for√ßar processamento completo
        print("‚ö° MODO INCREMENTAL:")
        print("  1. Inteligente - Para quando encontra vagas conhecidas")
        print("  2. For√ßado - Processa todas as p√°ginas mesmo com vagas conhecidas")
        print("Op√ß√£o (1-2, padr√£o: 1): ", end="")
        
        try:
            incremental_mode = int(input().strip() or "1")
            force_full = incremental_mode == 2
        except:
            force_full = False
        
        if force_full:
            print("üîÑ Modo for√ßado ativado - processar√° todas as p√°ginas")
        
        from src.scraper_optimized import scrape_catho_jobs_optimized
        jobs = await scrape_catho_jobs_optimized(
            max_concurrent_jobs=max_concurrent, 
            max_pages=max_pages,
            incremental=not force_full,  # Desabilita incremental se for√ßado
            show_compression_stats=True,
            enable_deduplication=True  # Deduplica√ß√£o sempre ativada
        )
    else:  # choice == 3
        print("INICIANDO COLETA M√ÅXIMA PERFORMANCE (POOL + TODAS OTIMIZA√á√ïES)...")
        print(f"{'='*60}")
        
        # Op√ß√£o de for√ßar processamento completo
        print("‚ö° MODO INCREMENTAL:")
        print("  1. Inteligente - Para quando encontra vagas conhecidas")
        print("  2. For√ßado - Processa todas as p√°ginas mesmo com vagas conhecidas")
        print("Op√ß√£o (1-2, padr√£o: 1): ", end="")
        
        try:
            incremental_mode = int(input().strip() or "1")
            force_full = incremental_mode == 2
        except:
            force_full = False
        
        if force_full:
            print("üîÑ Modo for√ßado ativado - processar√° todas as p√°ginas")
        
        from src.scraper_pooled import scrape_catho_jobs_pooled
        jobs = await scrape_catho_jobs_pooled(
            max_concurrent_jobs=max_concurrent, 
            max_pages=max_pages,
            incremental=not force_full,  # Desabilita incremental se for√ßado
            show_compression_stats=True,
            show_pool_stats=True,
            pool_min_size=2,
            pool_max_size=max_concurrent + 2,
            enable_deduplication=True  # Deduplica√ß√£o sempre ativada
        )
    
    if jobs:
        print(f"\n{'='*60}")
        print(f"DADOS COLETADOS: {len(jobs)} vagas encontradas")
        print(f"{'='*60}")
        
        # Aplicar filtros e an√°lise
        print("\nAplicando filtros e an√°lise...")
        job_filter = JobFilter()
        
        if filters_config:
            filtered_jobs = job_filter.apply_filters(jobs, filters_config)
            print(f"‚úì Filtros aplicados: {len(filtered_jobs)} vagas selecionadas")
        else:
            # Aplicar apenas an√°lise sem filtros
            filtered_jobs = job_filter.apply_filters(jobs, {})
            print(f"‚úì An√°lise aplicada a todas as {len(filtered_jobs)} vagas")
        
        jobs = filtered_jobs
        
        if jobs:
            print(f"\n{'='*60}")
            print(f"RESULTADO FINAL: {len(jobs)} vagas processadas")
            print(f"{'='*60}")
            
            # Salvando resultados
            save_results(jobs, filters_config)
            
            print(f"\n‚úÖ PROCESSAMENTO CONCLU√çDO COM SUCESSO!")
            print(f"üìÅ Arquivos organizados em subdiret√≥rios")
            
            # Preview dos resultados
            print(f"\nüî• PREVIEW DOS RESULTADOS:")
            if jobs:
                # Tecnologias mais demandadas
                all_techs = {}
                for job in jobs:
                    for tech in job.get('tecnologias_detectadas', []):
                        all_techs[tech] = all_techs.get(tech, 0) + 1
                
                if all_techs:
                    print(f"   üíª Top 5 tecnologias:")
                    for tech, count in sorted(all_techs.items(), key=lambda x: x[1], reverse=True)[:5]:
                        print(f"      - {tech}: {count} vagas")
                
                # An√°lise de n√≠veis
                niveis = {}
                for job in jobs:
                    nivel = job.get('nivel_categorizado', 'N√£o categorizado')
                    niveis[nivel] = niveis.get(nivel, 0) + 1
                
                if niveis:
                    print(f"   üìä Distribui√ß√£o por n√≠vel:")
                    for nivel, count in sorted(niveis.items(), key=lambda x: x[1], reverse=True)[:3]:
                        print(f"      - {nivel.replace('_', ' ').title()}: {count} vagas")
        else:
            print("\n‚ö† Nenhuma vaga atendeu aos crit√©rios de filtro especificados.")
            print("Tente ajustar os filtros ou executar sem filtros.")
    else:
        print("\n‚úó Nenhuma vaga foi encontrada no site.")


if __name__ == "__main__":
    asyncio.run(main())