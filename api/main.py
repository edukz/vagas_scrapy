"""
API REST para Catho Job Scraper

Sistema completo de API com FastAPI incluindo:
- Endpoints para controle de scraping
- Busca e consulta de dados
- Gerenciamento de cache
- Autentica√ß√£o JWT
- Documenta√ß√£o autom√°tica (Swagger/ReDoc)
- Background tasks
- Rate limiting
- Monitoramento
"""

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from typing import Optional, List, Dict, Any
import asyncio
import time
import os
import sys
from datetime import datetime, timedelta
import uuid

# Adicionar src ao path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Importar m√≥dulos do scraper
from src.core.scraper_pooled import scrape_catho_jobs_pooled
from src.systems.compressed_cache import CompressedCache
from src.systems.deduplicator import JobDeduplicator
from src.utils.filters import JobFilter

# Importar configura√ß√µes da API
from api.config import settings
from api.models import (
    ScrapingRequest, ScrapingResponse, ScrapingStatus,
    SearchRequest, SearchResponse, JobModel,
    CacheStatsResponse, SystemHealthResponse,
    TokenResponse, UserLogin
)
from api.auth import (
    authenticate_user, create_access_token, get_current_user,
    get_password_hash, verify_password
)
from api.tasks import scraping_task_manager
from api.rate_limiter import RateLimiter
from api.database import init_db

# Configurar rate limiter
rate_limiter = RateLimiter(requests_per_minute=60)

# Lifespan context manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gerencia ciclo de vida da aplica√ß√£o"""
    # Startup
    print("üöÄ Iniciando API REST...")
    await init_db()
    await scraping_task_manager.initialize()
    print("‚úÖ API pronta para receber requisi√ß√µes!")
    
    yield
    
    # Shutdown
    print("üîå Desligando API...")
    await scraping_task_manager.shutdown()
    print("üëã API finalizada!")

# Criar aplica√ß√£o FastAPI
app = FastAPI(
    title="Catho Job Scraper API",
    description="""
    API REST completa para o sistema de scraping de vagas do Catho.
    
    ## Funcionalidades
    
    * **Scraping** - Iniciar, parar e monitorar processos de scraping
    * **Busca** - Pesquisar vagas com filtros avan√ßados
    * **Cache** - Gerenciar cache e √≠ndices
    * **Estat√≠sticas** - Analytics e m√©tricas do sistema
    * **Autentica√ß√£o** - Sistema seguro com JWT
    
    ## Autentica√ß√£o
    
    A maioria dos endpoints requer autentica√ß√£o via Bearer Token.
    Use `/auth/token` para obter um token JWT.
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Middleware para rate limiting
@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    """Aplica rate limiting em todas as requisi√ß√µes"""
    client_ip = request.client.host
    
    if not rate_limiter.check_rate_limit(client_ip):
        return JSONResponse(
            status_code=429,
            content={"detail": "Too many requests. Please try again later."}
        )
    
    response = await call_next(request)
    return response

# ==================== ENDPOINTS DE AUTENTICA√á√ÉO ====================

@app.post("/auth/token", response_model=TokenResponse, tags=["Authentication"])
async def login(user_credentials: UserLogin):
    """
    Autenticar usu√°rio e obter token JWT
    
    - **username**: Nome de usu√°rio
    - **password**: Senha do usu√°rio
    
    Retorna um token JWT v√°lido por 30 minutos
    """
    user = authenticate_user(user_credentials.username, user_credentials.password)
    if not user:
        raise HTTPException(
            status_code=401,
            detail="Credenciais inv√°lidas",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user["username"]}, expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": settings.ACCESS_TOKEN_EXPIRE_MINUTES * 60
    }

# ==================== ENDPOINTS DE SCRAPING ====================

@app.post("/api/v1/scraping/start", 
         response_model=ScrapingResponse,
         tags=["Scraping"],
         summary="Iniciar novo processo de scraping")
async def start_scraping(
    request: ScrapingRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """
    Inicia um novo processo de scraping em background
    
    Par√¢metros:
    - **max_pages**: N√∫mero m√°ximo de p√°ginas para processar (1-20)
    - **max_concurrent_jobs**: Jobs simult√¢neos (1-5)
    - **incremental**: Se deve processar apenas dados novos
    - **enable_deduplication**: Se deve remover duplicatas
    - **filters**: Filtros opcionais (tecnologias, sal√°rio, etc)
    
    Retorna:
    - **task_id**: ID √∫nico para acompanhar o progresso
    - **status**: Status inicial do processo
    """
    # Validar limites
    if request.max_pages > 20:
        raise HTTPException(status_code=400, detail="M√°ximo de 20 p√°ginas permitido")
    
    if request.max_concurrent_jobs > 5:
        raise HTTPException(status_code=400, detail="M√°ximo de 5 jobs simult√¢neos")
    
    # Criar task ID √∫nico
    task_id = str(uuid.uuid4())
    
    # Criar task de scraping
    task = scraping_task_manager.create_task(
        task_id=task_id,
        user_id=current_user["username"],
        config=request.dict()
    )
    
    # Adicionar ao background
    background_tasks.add_task(
        scraping_task_manager.run_scraping,
        task_id,
        request
    )
    
    return ScrapingResponse(
        task_id=task_id,
        status="started",
        message=f"Scraping iniciado com ID {task_id}",
        started_at=datetime.now(),
        config=request.dict()
    )

@app.get("/api/v1/scraping/status/{task_id}",
         response_model=ScrapingStatus,
         tags=["Scraping"],
         summary="Verificar status de scraping")
async def get_scraping_status(
    task_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Verifica o status de um processo de scraping
    
    Retorna informa√ß√µes detalhadas sobre o progresso, incluindo:
    - Status atual (running, completed, failed)
    - Progresso (p√°ginas processadas, vagas encontradas)
    - Mensagens de erro (se houver)
    - Estat√≠sticas de performance
    """
    task = scraping_task_manager.get_task(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail="Task n√£o encontrada")
    
    # Verificar se usu√°rio tem acesso
    if task["user_id"] != current_user["username"] and current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Acesso negado")
    
    return task

@app.post("/api/v1/scraping/stop/{task_id}",
          tags=["Scraping"],
          summary="Parar processo de scraping")
async def stop_scraping(
    task_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    Para um processo de scraping em execu√ß√£o
    
    O processo ser√° interrompido de forma segura, salvando
    todos os dados coletados at√© o momento.
    """
    success = await scraping_task_manager.stop_task(task_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="Task n√£o encontrada ou j√° finalizada")
    
    return {"message": f"Scraping {task_id} parado com sucesso"}

@app.get("/api/v1/scraping/history",
         tags=["Scraping"],
         summary="Hist√≥rico de scraping")
async def get_scraping_history(
    limit: int = 10,
    offset: int = 0,
    current_user: dict = Depends(get_current_user)
):
    """
    Retorna hist√≥rico de processos de scraping do usu√°rio
    
    - **limit**: N√∫mero m√°ximo de registros
    - **offset**: Pagina√ß√£o
    """
    history = scraping_task_manager.get_user_history(
        user_id=current_user["username"],
        limit=limit,
        offset=offset
    )
    
    return {
        "total": len(history),
        "limit": limit,
        "offset": offset,
        "tasks": history
    }

# ==================== ENDPOINTS DE BUSCA ====================

@app.post("/api/v1/data/search",
          response_model=SearchResponse,
          tags=["Data"],
          summary="Buscar vagas")
async def search_jobs(
    request: SearchRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    Busca vagas no cache com filtros avan√ßados
    
    Filtros dispon√≠veis:
    - **companies**: Lista de empresas
    - **technologies**: Lista de tecnologias
    - **locations**: Lista de localiza√ß√µes
    - **salary_min**: Sal√°rio m√≠nimo
    - **date_from**: Data inicial
    - **date_to**: Data final
    - **limit**: M√°ximo de resultados
    - **offset**: Pagina√ß√£o
    """
    cache = CompressedCache()
    
    # Converter filtros para formato do cache
    criteria = {}
    if request.companies:
        criteria['companies'] = request.companies
    if request.technologies:
        criteria['technologies'] = request.technologies
    if request.locations:
        criteria['locations'] = request.locations
    if request.date_from:
        criteria['date_from'] = request.date_from
    if request.date_to:
        criteria['date_to'] = request.date_to
    
    # Buscar no cache
    results = cache.search_cache(criteria)
    
    # Aplicar pagina√ß√£o
    total = len(results)
    start = request.offset
    end = start + request.limit
    paginated_results = results[start:end]
    
    # Converter para modelo de resposta
    jobs = []
    for entry in paginated_results:
        # Ler dados completos do arquivo
        cached_data = await cache.get(entry.url)
        if cached_data and 'jobs' in cached_data:
            for job in cached_data['jobs']:
                jobs.append(JobModel(**job))
    
    return SearchResponse(
        total=total,
        limit=request.limit,
        offset=request.offset,
        jobs=jobs,
        query_time_ms=0  # TODO: Implementar medi√ß√£o
    )

@app.get("/api/v1/data/stats",
         tags=["Data"],
         summary="Estat√≠sticas gerais")
async def get_statistics(current_user: dict = Depends(get_current_user)):
    """
    Retorna estat√≠sticas gerais do sistema
    
    Inclui:
    - Total de vagas no cache
    - Top empresas
    - Top tecnologias
    - Distribui√ß√£o por localiza√ß√£o
    - Tend√™ncias temporais
    """
    cache = CompressedCache()
    
    stats = cache.get_cache_stats()
    top_companies = cache.get_top_companies(10)
    top_technologies = cache.get_top_technologies(10)
    
    return {
        "total_jobs": stats['index']['total_jobs'],
        "total_entries": stats['index']['total_entries'],
        "cache_size_mb": stats['compression']['total_cache_size_mb'],
        "compression_ratio": stats['compression']['average_compression_ratio'],
        "top_companies": [
            {"name": company, "count": count} 
            for company, count in top_companies
        ],
        "top_technologies": [
            {"name": tech, "count": count} 
            for tech, count in top_technologies
        ],
        "last_updated": stats['index'].get('last_updated')
    }

# ==================== ENDPOINTS DE CACHE ====================

@app.get("/api/v1/cache/stats",
         response_model=CacheStatsResponse,
         tags=["Cache"],
         summary="Estat√≠sticas do cache")
async def get_cache_stats(current_user: dict = Depends(get_current_user)):
    """
    Retorna estat√≠sticas detalhadas do cache
    
    Inclui informa√ß√µes sobre:
    - Tamanho e compress√£o
    - N√∫mero de entradas
    - Taxa de hit/miss
    - Idade dos dados
    """
    cache = CompressedCache()
    stats = cache.get_cache_stats()
    
    return CacheStatsResponse(
        total_entries=stats['index']['total_entries'],
        total_jobs=stats['index']['total_jobs'],
        cache_size_mb=stats['compression']['total_cache_size_mb'],
        compression_ratio=stats['compression']['average_compression_ratio'],
        oldest_entry=stats['index'].get('oldest_entry'),
        newest_entry=stats['index'].get('newest_entry'),
        last_updated=stats['index'].get('last_updated')
    )

@app.post("/api/v1/cache/clean",
          tags=["Cache"],
          summary="Limpar cache")
async def clean_cache(
    remove_duplicates: bool = True,
    remove_old: bool = False,
    max_age_days: int = 7,
    current_user: dict = Depends(get_current_user)
):
    """
    Limpa o cache removendo dados antigos ou duplicados
    
    Par√¢metros:
    - **remove_duplicates**: Remove vagas duplicadas
    - **remove_old**: Remove dados antigos
    - **max_age_days**: Idade m√°xima em dias (se remove_old=true)
    
    ‚ö†Ô∏è Requer privil√©gios de administrador
    """
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Apenas administradores podem limpar o cache")
    
    results = {
        "duplicates_removed": 0,
        "old_entries_removed": 0
    }
    
    if remove_duplicates:
        deduplicator = JobDeduplicator()
        results["duplicates_removed"] = deduplicator.clean_existing_files("data")
    
    if remove_old:
        # TODO: Implementar remo√ß√£o de dados antigos
        pass
    
    return {
        "message": "Cache limpo com sucesso",
        "results": results
    }

@app.post("/api/v1/cache/rebuild-index",
          tags=["Cache"],
          summary="Reconstruir √≠ndices")
async def rebuild_cache_index(
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """
    Reconstr√≥i os √≠ndices do cache para busca r√°pida
    
    Este processo pode demorar alguns minutos dependendo
    do tamanho do cache. Ser√° executado em background.
    
    ‚ö†Ô∏è Requer privil√©gios de administrador
    """
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Apenas administradores podem reconstruir √≠ndices")
    
    task_id = str(uuid.uuid4())
    
    # TODO: Adicionar ao background
    background_tasks.add_task(rebuild_index_task, task_id)
    
    return {
        "message": "Reconstru√ß√£o de √≠ndices iniciada",
        "task_id": task_id
    }

# ==================== ENDPOINTS DE SISTEMA ====================

@app.get("/api/v1/health",
         response_model=SystemHealthResponse,
         tags=["System"],
         summary="Health check")
async def health_check():
    """
    Verifica a sa√∫de do sistema
    
    Retorna o status de todos os componentes:
    - API
    - Cache
    - Scraper
    - Background tasks
    """
    cache = CompressedCache()
    
    # Verificar componentes
    components = {
        "api": "healthy",
        "cache": "healthy" if os.path.exists("data/cache") else "unhealthy",
        "scraper": "healthy",  # TODO: Verificar pool de conex√µes
        "background_tasks": "healthy" if scraping_task_manager.is_healthy() else "unhealthy"
    }
    
    overall_health = "healthy" if all(v == "healthy" for v in components.values()) else "degraded"
    
    return SystemHealthResponse(
        status=overall_health,
        timestamp=datetime.now(),
        version="1.0.0",
        uptime_seconds=int(time.time() - app.state.start_time) if hasattr(app.state, 'start_time') else 0,
        components=components
    )

@app.get("/api/v1/metrics",
         tags=["System"],
         summary="M√©tricas do sistema")
async def get_metrics(current_user: dict = Depends(get_current_user)):
    """
    Retorna m√©tricas detalhadas do sistema
    
    Inclui:
    - Taxa de requisi√ß√µes
    - Tempo de resposta
    - Uso de recursos
    - Estat√≠sticas de scraping
    
    ‚ö†Ô∏è Requer privil√©gios de administrador
    """
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Apenas administradores podem ver m√©tricas")
    
    # TODO: Implementar coleta de m√©tricas reais
    return {
        "requests_per_minute": rate_limiter.get_current_rpm(),
        "active_scraping_tasks": scraping_task_manager.get_active_count(),
        "total_jobs_scraped": 0,  # TODO: Implementar contador
        "average_response_time_ms": 0,  # TODO: Implementar medi√ß√£o
        "cache_hit_rate": 0,  # TODO: Implementar medi√ß√£o
    }

# ==================== ROOT ENDPOINT ====================

@app.get("/", tags=["Root"])
async def root():
    """
    Endpoint raiz com informa√ß√µes da API
    """
    return {
        "name": "Catho Job Scraper API",
        "version": "1.0.0",
        "documentation": "/docs",
        "redoc": "/redoc",
        "health": "/api/v1/health",
        "description": "API REST completa para scraping de vagas do Catho"
    }

# ==================== ERROR HANDLERS ====================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handler customizado para erros HTTP"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "timestamp": datetime.now().isoformat(),
            "path": request.url.path
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handler para erros n√£o tratados"""
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": str(exc) if settings.DEBUG else "An unexpected error occurred",
            "timestamp": datetime.now().isoformat(),
            "path": request.url.path
        }
    )

# ==================== STARTUP EVENT ====================

@app.on_event("startup")
async def startup_event():
    """Inicializa√ß√£o da aplica√ß√£o"""
    app.state.start_time = time.time()
    print(f"üöÄ API iniciada em: http://{settings.HOST}:{settings.PORT}")
    print(f"üìö Documenta√ß√£o: http://{settings.HOST}:{settings.PORT}/docs")

# ==================== HELPER FUNCTIONS ====================

async def rebuild_index_task(task_id: str):
    """Task para reconstruir √≠ndices em background"""
    cache = CompressedCache()
    count = cache.rebuild_index()
    # TODO: Salvar resultado da task
    return count

# ==================== MAIN ====================

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "api.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info" if not settings.DEBUG else "debug"
    )